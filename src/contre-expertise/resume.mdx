

[La Commission de l'intelligence artificielle](https://www.info.gouv.fr/communique/comite-de-lintelligence-artificielle), créée par le gouvernement français et présidée par Mme Bouverot et M. Aghion, a pour mission de positionner la France en leader face aux enjeux de l’IA pour les années à venir. [Son rapport final](https://www.bercynumerique.finances.gouv.fr/le-rapport-ia-notre-ambition-pour-la-france), publié le 13 mars 2024, s'avère être une occasion manquée, malgré l’importance extrême de ce document pour l'avenir technologique et économique de la France. Notre contre-expertise, soutenue par \[X\] experts reconnus et \[Y\] organisations, met en lumière de nombreuses lacunes et biais méthodologiques dans ce document.

Le rapport de la Commission représente un travail conséquent qui invite à la formation des personnes et incite à faire vivre un débat. Dans cette perspective, notre analyse se concentre délibérément sur ses aspects problématiques. Cette approche critique vise à combler les manques importants que nous avons identifiés, sans pour autant nier l'existence d'éléments très pertinents dans le rapport original.

**Au cœur de notre critique se trouve le non-respect des standards scientifiques de ce rapport**, qui écarte sans aucune justification l'avis d'une grande partie des experts en IA. Face à une situation où de nombreux chercheurs éminents, dont une majorité des experts en sécurité de l'IA, alertent sur des risques potentiellement catastrophiques à court et moyen terme, **la Commission fait le choix de les ignorer**. Cette approche compromet la capacité de la France à anticiper et gérer les défis majeurs posés par l'IA, mettant ainsi en péril la sécurité nationale et l'avenir de notre société.

Notre analyse révèle des défaillances systématiques dans le rapport, que nous pouvons résumer en sept critiques principales :

1. Silence injustifié sur les avertissements des experts en IA et sur la littérature scientifique existante.  
2. Aucune anticipation des développements futurs de l'IA, même à court terme.  
3. Omission des considérations principales sur la sécurité de l'IA.  
4. Négligence des risques liés aux algorithmes de recommandation, déjà bien présents et influents  
5. Sélection des données et des faits pour soutenir une vision excessivement optimiste.  
6. Minimisation systématique des risques identifiés, notamment en matière d'emploi et de cybersécurité.  
7. Insuffisance de rigueur en comparaison aux rapports similaires d'autres pays.

Ces défaillances sont illustrées à de nombreuses reprises, dont voici quelques exemples :

- **Emploi :** Le rapport manipule ouvertement les données sur l'impact de l'IA. Dans un premier temps, il reconnaît que sa première approche n’est pas  concluante, mais affirme quelques phrases plus loin qu'elle montre un 'effet positif sur l'emploi'. Ensuite, il cite une étude prévoyant un risque de remplacement de 30% du travail par l'IA, mais l'ignore immédiatement pour conclure, sans justification, à un effet positif.  
- **Cybersécurité :** Alors que l'IA surpasse déjà 88% des pirates humains dans certaines compétitions, le rapport disqualifie ces risques en deux phrases, sans aucune référence.  
- **Risques existentiels :** Le rapport ridiculise les avertissements sur les risques existentiels de l'IA sans aucune justification scientifique, tout en démontrant une méconnaissance du sujet en allant jusqu'à confondre la [déclaration du Center for AI Safety](https://www.safe.ai/work/statement-on-ai-risk) signée par des centaines d’experts de premier plan avec la [lettre ouverte](https://futureoflife.org/open-letter/pause-giant-ai-experiments/) d’une autre institution.  
- **Création artistique :** Sans citer aucune source ni référence, le rapport affirme que « L'IA ne met pas en danger l'originalité de la création ». Cette déclaration ignore les rapports alarmants qui émergent sur l'impact de l'IA dans le domaine artistique, et occulte les transformations profondes qu’elle impose dès aujourd’hui à ce secteur.  
- **Absence d'anticipation :** Le rapport évalue systématiquement les risques en se basant uniquement sur les capacités passées des IA, supposant implicitement un arrêt soudain du progrès technologique. Cette approche amplifie tous les problèmes précédents : les risques déjà sous-estimés pour l'emploi, la cybersécurité et la création artistique sont décuplés si l'on considère l'évolution rapide et continue des capacités de l'IA.

Ces manquements s'expliquent en grande partie par la composition même de la Commission, marquée par **des conflits d'intérêts majeurs** et un **manque de diversité d'opinions et d'expertises.** La Commission ne compte aucun expert en sécurité de l'IA et est dominée par des représentants de l'industrie favorables à, et favorisés par, un développement accéléré et peu régulé de l'IA.

Face à cette situation alarmante, nous appelons à :

1. Un remaniement de la Commission pour éliminer les conflits d'intérêts et garantir une  diversité d’expertise.  
2. La consultation urgente d'experts en sécurité de l'IA.  
3. À la suite de cette consultation, la rédaction d'un addendum traitant des risques ignorés.  
4. L'ouverture d'un débat public éclairé sur l'avenir de l'IA en France.

L'accueil par la France du Sommet pour l'action sur l'IA début 2025 offre l'occasion de prendre une position de leadership sur les enjeux de sécurité de l'IA. **Ces enjeux ne représentent rien de moins que l'avenir de notre société et, potentiellement, de l'humanité toute entière.** Il est impératif que la France prenne la mesure réelle des défis et des dangers que pose cette technologie, et agisse en conséquence. 

Cette contre-expertise se veut une réponse concrète à l'appel au débat public mentionné dans le rapport de la Commission (p.7), initiant ainsi un dialogue critique et constructif sur l'avenir de l'IA.
