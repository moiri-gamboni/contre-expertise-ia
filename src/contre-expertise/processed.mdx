import { Sidenote } from '@/components/Sidenote'
import { ReportSection } from '@/components/ReportSection'

<ReportSection id="introduction" number="1" navTitle="Introduction" className='print:pt-0'>
L'intelligence artificielle (IA) transforme rapidement notre société, avec des implications profondes pour l'avenir[[1], [2]](https://paperpile.com/c/V0trIA/PKULH+Ad0eu). Les décisions prises aujourd'hui en matière de développement et de régulation de l'IA auront des impacts concrets sur l'économie, la sécurité, l'influence internationale de la France, ainsi que sur son industrie, sa recherche scientifique, l'emploi et la vie privée des citoyens. Il est donc essentiel d'examiner attentivement les bases sur lesquelles ces décisions sont prises.

Prenant acte de l’importance de ces impacts, le gouvernement français crée en septembre 2023 le Comité - depuis devenu Commission - de l’Intelligence Artificielle. La Commission est sommée, sous six mois, de produire un rapport censé guider les politiques nationales en matière d'IA pour les années à venir [[3, p. 128]](https://paperpile.com/c/V0trIA/3d3fm/?locator=128). Ce rapport [[3]](https://paperpile.com/c/V0trIA/3d3fm) est publié le 13 mars 2024 et intitulé <span style={{textDecoration: "underline"}}>IA: Notre ambition pour la France</span>. 

Le présent document propose une analyse critique approfondie du rapport de la Commission, s'inscrivant dans la lignée d'autres critiques notables, notamment celles exprimées par l’UGICT-CGT [[4]](https://paperpile.com/c/V0trIA/OxGj7) et le Centre pour la Sécurité de l’IA [[5]](https://paperpile.com/c/V0trIA/09bm). Nous mettons en évidence des lacunes majeures et des conflits d'intérêts qui, s’ils restaient ignorés, pourraient avoir des implications considérables pour l'avenir de la France et de l'humanité. **Notre objectif principal est de créer les conditions d'un débat sain et équilibré autour de l'IA, qui ne soit plus capturé par des lobbys puissants et des intérêts privés et s’appuie sur les connaissances scientifiques modernes en IA.**

Notre analyse se divise en trois parties principales :



1. Analyse critique du rapport de la Commission
2. Évaluation de la composition et du fonctionnement de la Commission
3. Recommandations pour adresser les lacunes identifiées

Dans la première partie, nous examinerons en détail :



1. Les omissions critiques du rapport
2. La minimisation et la distorsion des risques
3. La négligence des préoccupations citoyennes
4. La différence de qualité d’analyse comparée à d'autres rapports nationaux et internationaux

La seconde partie se concentrera sur l'analyse de la composition de la Commission, mettant en évidence les potentiels conflits d'intérêts et leurs implications sur les conclusions du rapport.

Enfin, nous proposerons des recommandations concrètes visant à combler les lacunes identifiées et à améliorer le processus d'élaboration des politiques en matière d'IA.

Cette contre-expertise se veut une contribution critique et constructive au débat national sur l'IA. En mettant en lumière les faiblesses du rapport de la Commission et en proposant des pistes d'amélioration, nous cherchons à promouvoir une approche plus rigoureuse et transparente des enjeux de l'IA, dans l'intérêt à court terme de la société française.
</ReportSection>

<ReportSection id="methodologie-et-approche" number="2" navTitle="Méthodologie et approche">
Cette contre-expertise a été réalisée entièrement par un groupe de citoyens bénévoles possédant diverses expertises, sous la direction de Maxime Fournes, expert en Intelligence Artificielle. Notre approche a été principalement qualitative, axée sur une analyse critique approfondie du rapport de la Commission de l'IA.

**Engagement bénévole :**

- Ce travail a été effectué intégralement par des bénévoles sur leur temps libre (voir section “les auteurs”).

- L'effort collectif représente environ 300 heures de travail, réparties sur plusieurs mois.

- Nous avons délibérément limité notre temps d'analyse afin de publier cette critique suffisamment en amont du sommet de l'IA, permettant ainsi une réflexion et un débat éclairés.

**Objectifs :**

- Évaluer la rigueur scientifique et l'exhaustivité du rapport de la Commission

- Identifier les lacunes et les biais potentiels

- Proposer des recommandations pour une approche plus complète et équilibrée

**Sources et collecte de données :**

- Analyse détaillée du rapport de la Commission de l'IA

- Revue de la littérature scientifique pertinente sur la sécurité de l'IA

- Consultation de chercheurs en sécurité de l'intelligence artificielle

**Processus d'analyse :**

1. Lecture critique du rapport de la Commission

2. Identification des points clés et des lacunes potentielles

3. Comparaison avec la littérature scientifique existante

4. Consultation d'experts pour valider nos observations

5. Synthèse des résultats et formulation de recommandations

**Validation :**

À l'issue de la rédaction initiale, le document a été soumis à [X] experts indépendants pour relecture et validation. **Le processus de validation s'est concentré sur la partie analytique du rapport, excluant les recommandations qui n'engagent que les auteurs principaux**.

**Niveaux de participation :**



1. **Validation : **L'expert confirme l'exactitude et la pertinence de l'analyse présentée. Cette validation peut s'appliquer à l'ensemble du document ou à des sections spécifiques relevant de l'expertise de l'expert.
2. **Soutien :** L'expert approuve les principales conclusions de l'analyse, tout en exprimant des réserves sur certains points spécifiques. Le soutien peut concerner l'ensemble du document ou des sections spécifiques.

Les experts ont eu la possibilité de choisir les sections qu'ils souhaitaient examiner, en fonction de leur domaine d'expertise. Chaque expert a eu l'opportunité d'ajouter un bref commentaire personnel pour expliciter sa position ou ses éventuelles réserves.

**Limitations :**

- Cette contre-expertise a été réalisée de manière indépendante, sans accès aux données brutes ou aux délibérations de la Commission de l'IA.

- Les ressources et le temps disponibles pour cette analyse étaient limités par rapport à ceux de la Commission de l’IA.
</ReportSection>

<ReportSection id="analyse-critique-du-rapport" number="3" navTitle="Analyse critique du rapport">
### 3.1 Vue d’ensemble

Le rapport de la Commission de l'IA [[3, p. 128]](https://paperpile.com/c/V0trIA/3d3fm/?locator=128) présente des lacunes profondes et multiples qui compromettent gravement sa crédibilité et son utilité en tant que document d'orientation stratégique pour la France. Notre analyse révèle une approche biaisée, manquant de rigueur scientifique et ignorant délibérément certains risques majeurs liés au développement rapide de l'IA.

Premièrement, le rapport souffre d'omissions critiques. Il passe sous silence le domaine scientifique de la sécurité de l'IA, établi depuis plus de quinze ans [[6]](https://paperpile.com/c/V0trIA/Xy4dz). Il ignore les avertissements de la majorité des experts concernant les risques existentiels, y compris ceux émis par des figures emblématiques comme Stuart Russell [[7]](https://paperpile.com/c/V0trIA/zp3pc), Yoshua Bengio [[8], [9]](https://paperpile.com/c/V0trIA/hnEIi+JT1YF) et Geoffrey Hinton [[10]](https://paperpile.com/c/V0trIA/c8QZ8). De plus, le rapport fait preuve d'un manque important d'anticipation, se concentrant principalement sur les impacts des technologies actuelles sans considérer sérieusement les défis potentiels posés par les innovations à venir.

Deuxièmement, le rapport minimise systématiquement les risques qu'il daigne aborder. Cette minimisation se manifeste à travers une déformation du contexte historique et actuel de l'IA, créant une fausse impression de continuité et de maîtrise technique. Le traitement des risques spécifiques, notamment en matière d'emploi et de cybersécurité, est particulièrement biaisé et fallacieux. Le rapport emploie des formulations orientées ou arguments rhétoriques pour discréditer les scénarios les plus alarmants sans en adresser la substance, et façonne ainsi une perception des enjeux rassurante mais trompeuse.

Troisièmement, l'analyse comparative avec d'autres initiatives similaires met en lumière les déficiences méthodologiques du rapport. La sélection des experts, la formulation des conclusions, et l'incapacité à explorer divers scénarios crédibles témoignent d'une approche peu rigoureuse et potentiellement orientée.

Les lacunes identifiées ne sont pas de simples erreurs. Elles reflètent une défaillance systématique dans l'évaluation des risques et des opportunités liés à l'IA. Il est impératif que la France prenne la mesure réelle des défis posés par l'IA et adopte une approche plus rigoureuse, éthique et responsable pour guider son développement futur.


### 3.2 Omissions Critiques

Le rapport de la Commission est marqué par des omissions systématiques qui compromettent gravement sa crédibilité et son utilité. Ces lacunes ne semblent pas être de simples oublis, mais plutôt s’inscrire dans une démarche cohérente visant à minimiser les dangers et les défis posés par l'IA.

Parmi ces omissions, on note l'absence de mention du domaine scientifique de la sécurité de l'IA (aussi appelé “sûreté de l’IA”), l'occultation des avertissements de la majorité des experts concernant les risques, et un silence assourdissant sur les scénarios les plus graves, y compris les risques catastrophiques et existentiels. De plus, le rapport fait preuve d'un manque flagrant d'anticipation et de prospective, même à très court terme. Cette approche myope semble délibérément employée pour sous-estimer les dangers imminents en ignorant les avancées rapides et prévisibles du domaine. L'absence de mention du paradigme émergent des agents autonomes, anticipé depuis plus d'un an, illustre l'obsolescence du rapport dès sa publication il y a quelques mois.

Examinons maintenant en détail chacune de ces omissions critiques et leurs implications.


#### 3.2.1 Absence totale de mention de la sécurité de l'IA. 

Ce champ de recherche, établi il y a plus de quinze ans, se consacre spécifiquement à l'étude des risques potentiels liés au développement de l'IA, aux moyens de les atténuer, ainsi qu'aux questions fondamentales de compréhension et de contrôle des intelligences artificielles [[6]](https://paperpile.com/c/V0trIA/Xy4dz). Des institutions renommées telles que le Centre for the Study of Existential Risk à Cambridge [[11]](https://paperpile.com/c/V0trIA/BtfX), le Center for AI Safety (CAIS) [[12]](https://paperpile.com/c/V0trIA/qdJY), et le Center for Human-Compatible AI (CHAI) à Berkeley [[13]](https://paperpile.com/c/V0trIA/mcNh) sont à l'avant-garde de ces recherches. De plus, les principaux laboratoires d'IA comme OpenAI [[14], [15]](https://paperpile.com/c/V0trIA/oaxP+ouOj), DeepMind [[16], [17], [18]](https://paperpile.com/c/V0trIA/oKud+n7qT+oFEj) et Anthropic [[19], [20]](https://paperpile.com/c/V0trIA/42xA+YXCg) ont constitué des équipes dédiées à la sécurité de l'IA, soulignant l'importance croissante de ce domaine. Cette discipline est désormais enseignée dans des universités de premier plan à travers le monde, notamment à Stanford [[21]](https://paperpile.com/c/V0trIA/nGAW), au MIT [[22]](https://paperpile.com/c/V0trIA/Ld7M), à Berkeley [(DeCal )](https://paperpile.com/c/V0trIA/O1yu), et à l'ETH Zurich [[23]](https://paperpile.com/c/V0trIA/7Otq), ainsi que dans des écoles parmi les plus prestigieuses en France, telles que l'École Normale Supérieure [[24]](https://paperpile.com/c/V0trIA/IpSsj) et l'École Polytechnique [[25]](https://paperpile.com/c/V0trIA/lLTSI).

Parmi les accomplissements majeurs de ce domaine, on peut citer le développement de techniques d'alignement du comportement sur les valeurs humaines, notamment l'apprentissage par renforcement à partir de feedback humain (RLHF) [[26], [27]](https://paperpile.com/c/V0trIA/pvRc+nZwI). Cette approche est aujourd'hui le mécanisme principal permettant de rendre les grands modèles de langage utilisables et plus alignés avec les intentions humaines. L'identification de problèmes fondamentaux comme le "reward hacking" [[28]](https://paperpile.com/c/V0trIA/Pv5mO) a également permis de mieux comprendre les défis de l'alignement. Des avancées significatives ont été réalisées dans l'interprétabilité mécanistique [[29]](https://paperpile.com/c/V0trIA/b0Bl), avec des progrès notables d'Anthropic [[30]](https://paperpile.com/c/V0trIA/URfna) qui pourraient ouvrir la voie à une compréhension plus profonde du fonctionnement interne des modèles d'IA. Enfin, des analyses et études prospectives [[31], [32]](https://paperpile.com/c/V0trIA/Auk9N+Kp1pE) approfondies ont permis d'identifier et d'évaluer un large éventail de risques potentiels liés aux systèmes d'IA actuels et futurs [[33]](https://paperpile.com/c/V0trIA/6LW2Z).

Cependant, nous faisons les constats suivants :



1. Aucune mention n'est faite de ce domaine de recherche ni de ses conclusions.
2. Aucun des experts reconnus en sécurité de l'IA que nous avons consultés n'a été approché pour contribuer à ce rapport.

Ces omissions constituent une lacune stratégique majeure. En ignorant des connaissances essentielles pour l'évaluation des risques liés à l'IA, le rapport compromet la pertinence de ses recommandations. Cette approche soulève des inquiétudes quant à l'exhaustivité de l'expertise mobilisée et suggère que la stratégie nationale en matière d'IA pourrait ne pas prendre en compte certains des risques les plus sérieux identifiés par la communauté scientifique spécialisée.


#### 3.2.2 Omission des avertissements de la communauté scientifique sur les risques systémiques et catastrophiques liés à l'IA. 

Sections du rapport concernées: 1.2

Malgré ce que suggère le silence du rapport à leur sujet, ces préoccupations ne sont pas marginales. Au contraire, des études récentes montrent qu'une majorité des experts du domaine considère que les IA présentent un risque réel d'extinction pour l'humanité [[34]](https://paperpile.com/c/V0trIA/ghKix). Cette position est soutenue par des centaines de figures emblématiques du domaine [[35]](https://paperpile.com/c/V0trIA/eRWRE) telles que Yoshua Bengio et Geoffrey Hinton, co-récipients du prestigieux Prix Turing pour leurs travaux sur les réseaux de neurones profonds [[36], [37]](https://paperpile.com/c/V0trIA/npzJ+1Rxe).

En mai 2023, le Center for AI Safety a publié une déclaration [[35]](https://paperpile.com/c/V0trIA/eRWRE), signée par plus de 350 experts de premier plan, affirmant que "l'atténuation du risque d'extinction par l'IA devrait être une priorité mondiale". Cette déclaration a suivi une autre lettre ouverte, _Pause Giant AI Experiments_ [[38]](https://paperpile.com/c/V0trIA/WW03B), qui a recueilli plus de 33 000 signatures, entre autres d’experts en intelligence artificielle venant du milieu académique comme industriel, démontrant l'ampleur de ces préoccupations au sein de la communauté scientifique et au-delà.

La section "1.2 Faut-il avoir peur de l'IA ?" (p. 31) du rapport tourne en ridicule ces craintes sans aucune justification ou argument. Elle va même jusqu'à déformer les faits, confondant différentes lettres ouvertes et minimisant leur portée. Par exemple, le rapport fait référence à la déclaration du Center for AI Safety en affirmant qu'elle n'a recueilli que 60 signatures d'experts.

Il est important de noter qu'il existe également des scientifiques qui considèrent ces inquiétudes comme exagérées. Cependant, face à une telle controverse impliquant des risques potentiellement catastrophiques, il serait crucial d'étudier explicitement et rigoureusement les deux positions. Le rapport, en ignorant complètement l'un des côtés du débat, ne permet pas une évaluation équilibrée et approfondie des enjeux.

Cette représentation erronée et ce rejet des préoccupations légitimes de la communauté scientifique soulèvent une question fondamentale : comment un rapport censé guider la politique nationale en matière d'IA peut-il ignorer délibérément les avertissements alarmants d’une grande partie des experts du domaine ? En écartant ces voix sans aucune justification valable, le rapport prive les décideurs et le public d'informations essentielles pour comprendre l'ampleur réelle des défis posés par l'IA avancée.


#### 3.2.3 Aucune anticipation des développements de l'IA, même à court terme 

Sections du rapport concernées : 1.4

Bien que la Commission reconnaisse que l'IA connaîtra "de nouvelles avancées rapides et de grande ampleur" (p. 12), son analyse se limite principalement aux impacts des technologies actuelles, négligeant ainsi les défis potentiels posés par les innovations à venir. Cette lacune, déjà pointée par le Centre pour la Sécurité de l'IA (CeSIA) dans une analyse détaillée [[5]](https://paperpile.com/c/V0trIA/09bm), n'a reçu aucune réponse de la part de la Commission.


![alt_text](images/image1.png "image_tooltip")


_Performance des modèles d'IA sur divers benchmarks de 2000 à 2024, comprenant la vision par ordinateur (MNIST, ImageNet), la reconnaissance vocale (Switchboard), la compréhension du langage naturel (SQuAD 1.1, MNLU, GLUE), l'évaluation générale des modèles de langage (MMLU, Big Bench, et GPQA), et le raisonnement mathématique (MATH). De nombreux modèles dépassent le niveau de performance humaine (ligne noire solide). Kiela, D., Thrush, T., Ethayarajh, K., & Singh, A. (2023) 'Plotting Progress in AI'._

L'IA évolue à une vitesse sans précédent [[39], [40]](https://paperpile.com/c/V0trIA/zJYT7+oULXh), et le rapport récent du UK AI Safety Institute [[41]](https://paperpile.com/c/V0trIA/2ept8) reconnaît la possibilité que cette progression "extrêmement rapide" (p. 9) se poursuive dans un avenir proche. Ne pas adopter une approche prospective dans un tel contexte n'a aucun sens et compromet sérieusement la pertinence même à court terme des recommandations du rapport. Cette absence d'anticipation est particulièrement frappante dans le cas du changement de paradigme vers des systèmes d'IA plus autonomes, un développement majeur que la Commission a complètement manqué. Ces systèmes, capables d'exécuter de longues séries d'actions avec très peu de supervision humaine, soulèvent des questions d'ordre sociétal et des risques qualitativement différents des outils d'IA générative actuels [[42]](https://paperpile.com/c/V0trIA/7UVQb). Des projets comme AutoGPT [[43]](https://paperpile.com/c/V0trIA/ccDcs), Devin [[44]](https://paperpile.com/c/V0trIA/QqYJn), Genie [[45]](https://paperpile.com/c/V0trIA/xkIrC) ou encore AI Scientist [[46]](https://paperpile.com/c/V0trIA/ybkz), qui étaient prévisibles depuis plus d'un an, illustrent cette transformation fondamentale. Le fait que le rapport ne mentionne même pas cette tendance témoigne d'un manque alarmant de compréhension des développements en cours et à venir dans le domaine.


![alt_text](images/image2.png "image_tooltip")


_Figure illustrative extraite de l’article “Pour une IA française tournée vers l’avenir” du Centre pour la Sécurité de l’IA (CeSIA) [[5]](https://paperpile.com/c/V0trIA/09bm). Cette figure illustre le changement de paradigme actuel en IA, en comparant l'approche statique du rapport à l'évolution rapide des systèmes autonomes et leurs impacts systémiques potentiels, soulignant ainsi l'importance d'anticiper ces développements._

Il est important de noter que cette approche non prospective présente un intérêt stratégique : celui de minimiser les risques liés à l'IA par omission. En effet, tous les risques sont amplifiés par le développement des capacités des modèles. Si les avancées s'arrêtaient aujourd'hui, comme le suppose implicitement la Commission, alors les risques seraient effectivement bien moindres. Cependant, cette hypothèse est en contradiction flagrante avec la réalité du domaine [[39], [40]](https://paperpile.com/c/V0trIA/zJYT7+oULXh) et avec les propres déclarations de la Commission sur les "avancées rapides et de grande ampleur" (p. 12) à venir.

Cette absence d'anticipation se reflète particulièrement dans l'analyse des effets de l'IA sur l'emploi. La Commission cherche à anticiper ces effets en s'appuyant sur une enquête de l'INSEE [[47]](https://paperpile.com/c/V0trIA/lK3GH) comparant des entreprises ayant adopté l'IA à celles ne l'ayant pas fait entre 2016 et 2021. Or, cette période précède l'explosion de popularité de l'IA générative, survenue en 2022 avec ChatGPT. Il est peu probable que l'adoption de l'IA générative par les entreprises engendre les mêmes conséquences que l'intégration des technologies d'IA dite "étroite" ces dernières années. De plus, l'impact de l'adoption de technologies d'IA encore plus avancées dans les années à venir pourrait être bien plus important.

Paradoxalement, bien que le rapport de la Commission souligne que les études sur l'impact de l'IA sur l'emploi "ne permettent pas encore de conclure à un effet sur un horizon de quelques années", elle n'hésite pas à affirmer "un effet positif de l'IA sur l'emploi"(p.44,45). Cette conclusion hâtive, basée sur des données obsolètes et sans prise en compte des développements futurs, illustre parfaitement les dangers d'une approche non prospective dans un domaine en évolution aussi rapide que l'IA.


#### 3.2.5 Négligence des risques liés aux algorithmes de recommandation




### 3.3 Minimisation et déformation des risques

Le rapport de la Commission de l'IA ne se contente pas d'omettre des informations cruciales ; il présente également une vision déformée et minimisée des risques qu'il traite. Cette section analyse en détail les différentes techniques employées pour sous-estimer l'ampleur et la gravité des défis posés par le développement rapide de l'IA. Nous examinerons d'abord comment le rapport déforme le contexte historique et actuel de l'IA, créant ainsi une fausse impression de continuité et de maîtrise. Ensuite, nous mettrons en lumière le traitement biaisé de risques spécifiques, notamment en matière d'emploi et de cybersécurité. Enfin, nous analyserons les techniques de rhétorique fallacieuses utilisées pour discréditer les scénarios les plus alarmants et façonner une perception rassurante mais trompeuse des enjeux.


#### 3.3.1 Déformation historique et contextuelle

Sections du rapport concernées: Introduction p.17

Le rapport de la Commission présente une vision déformée de l'histoire et du contexte actuel de l'IA, ce qui conduit à une minimisation des risques et des défis posés par les développements récents.

**Présentation biaisée de l'histoire de l'IA**

Le rapport décrit l'IA comme une technologie mature avec une longue histoire, ce qui est trompeur. Cette perspective confond l'histoire du champ de recherche avec celle des technologies elles-mêmes. En réalité :



1. L'IA en tant que champ de recherche existe depuis les années 1950 [[48]](https://paperpile.com/c/V0trIA/58rlx).
2. Les paradigmes actuels, basés sur l'apprentissage profond et les grands modèles de langage (_LLM_, voir encart), sont extrêmement récents - à peine plus d'une décennie pour le succès de l’apprentissage profond qui a provoqué un regain de la recherche en IA [[49]](https://paperpile.com/c/V0trIA/VThFP), et seulement quelques années pour les _LLM [[50]](https://paperpile.com/c/V0trIA/kqmAC)_.

_[Encart: LLMs]_

Les _Large Language Models_ (_LLM_) sont des modèles d'intelligence artificielle entraînés à prédire le prochain élément dans un texte en fonction du contexte, générant ainsi du contenu cohérent. Ils utilisent des estimations probabilistes et sont capables de comprendre et de produire du langage humain, ce qui les rend particulièrement polyvalents pour diverses applications. Cependant, leur complexité et leur opacité posent des défis en termes de compréhension et de contrôle. Pour en savoir plus sur les _LLMs_, consultez l'annexe D.

[Fin de l’encart]

Cette présentation biaisée minimise le tournant constitué par les développements actuels. Les modèles d'IA contemporains représentent une rupture fondamentale avec les paradigmes précédents :



* Ils reposent sur des approches radicalement différentes (apprentissage profond vs approches symboliques).
* Ils manifestent des capacités émergentes, c'est-à-dire des aptitudes qui n'ont pas été explicitement programmées ou entraînées [[51]](https://paperpile.com/c/V0trIA/Mm57s). Il n'y a aucun moyen de savoir si la prochaine génération de modèles ne développera pas soudainement des capacités surhumaines en piratage informatique ou en ingénierie génétique, par exemple, avant même que ces modèles ne soient entraînés et testés.
* Leur vitesse d'évolution et d'amélioration est incomparable avec les systèmes précédents [[40], [52]](https://paperpile.com/c/V0trIA/PG4SP+oULXh).

**Représentation trompeuse des modèles actuels**

Le rapport sous-estime la complexité et l'imprévisibilité des modèles d'IA actuels, en particulier des _LLM_. Cette représentation trompeuse se manifeste de plusieurs façons :



1. Surestimation de notre compréhension :
    * Les _LLM _sont fondamentalement opaques [[53]](https://paperpile.com/c/V0trIA/d33wU). Les algorithmes menant d'une entrée à un résultat sont invisibles, même pour leurs créateurs.
    * Un nouveau champ de recherche, l'interprétabilité [[29], [54]](https://paperpile.com/c/V0trIA/nfPl+b0Bl), a dû être créé pour tenter de comprendre le fonctionnement interne de ces modèles. L’interprétabilité n’en est qu’à ses balbutiements et les experts en interprétabilité mécanistique rappellent que leurs travaux encore exploratoires ne devraient pas être confondus avec une solution finie au problème de l’opacité [[29]](https://paperpile.com/c/V0trIA/b0Bl).
2. Exagération de notre capacité de contrôle :
    * Les _LLM _ne sont pas "programmés" au sens traditionnel, mais plutôt "cultivés" à travers un processus d'entraînement sur d'énormes quantités de données. Contrairement à un logiciel classique où chaque fonction est explicitement codée, les comportements d'un _LLM _émergent de manière complexe et imprévisible à partir de son apprentissage.
    * Ils développent des capacités émergentes souvent découvertes après leur mise à disposition au public [[51]](https://paperpile.com/c/V0trIA/Mm57s).
    * Certains comportements potentiellement dangereux et indésirables ont été observés, comme le mensonge [[55]](https://paperpile.com/c/V0trIA/6mKdU), la manipulation [[56]](https://paperpile.com/c/V0trIA/sRfym), et le piratage informatique [[57], [58], [59], [60]](https://paperpile.com/c/V0trIA/WhQyF+g9wcP+L5AsK+AgWnR).
3. Minimisation de la complexité et de l'imprévisibilité :
    * Les capacités des LLM augmentent rapidement et de manière non linéaire [[61]](https://paperpile.com/c/V0trIA/FZgwJ), menant vers des risques d’augmentation brusque et incontrôlée des capacités.
    * Les expériences du passé éclairent peu sur les capacités futures de ces systèmes. Il est, d'une part, difficile d'anticiper quelles capacités émergentes vont survenir lors des prochaines générations de modèles. D'autre part, la continuité des _scaling laws [[62]](https://paperpile.com/c/V0trIA/3HmY)_, les lois prédisant l'amélioration des _LLMs _en fonction de leur taille et de la quantité de calcul investie pour leur entraînement, fiable jusqu'à présent, doit plutôt nous faire anticiper une amélioration des capacités des modèles plutôt qu'une stagnation [[63]](https://paperpile.com/c/V0trIA/fObp).

Le rapport de la Commission présente les IA actuelles comme de simples "outils" utilisés depuis des décennies, alors qu'elles résultent d'un paradigme extrêmement récent, marqué par un effort sans précédent pour créer des machines pensantes autonomes, potentiellement capables de surpasser l'intelligence humaine [[64]](https://paperpile.com/c/V0trIA/YieF9). Cette représentation trompeuse minimise les risques et les défis uniques posés par ces nouvelles technologies, dans un contexte de forte concurrence internationale qui fait peu de cas des dangers inhérents à l'intégration rapide de ces systèmes dans de nombreux aspects de la société.


#### 3.3.2 Traitement biaisé des risques spécifiques


##### 3.3.2.1 Emploi

Sections du rapport concernées: a, b, c

L'analyse des effets de l'IA sur l'emploi présentée dans le rapport de la Commission est fondamentalement erronée, au vu de deux problèmes majeurs :

**Hypothèse erronée d'un arrêt du progrès technologique. **

Le rapport suppose implicitement que le progrès en IA s'arrêtera au niveau actuel, ne considérant pas l'impact de modèles plus avancés que GPT-4. Cette hypothèse conduit à une sous-estimation systématique des risques :



* Elle ignore la rapidité des avancées en IA et les objectifs affichés des laboratoires de recherche, qui visent à créer une intelligence artificielle générale (AGI) [[64]](https://paperpile.com/c/V0trIA/YieF9). Une AGI, par déduction d'après la définition d’OpenAI, aurait la capacité d’automatiser l’essentiel du travail humain, cognitif dans un premier temps, et manuel à terme.
* Bien qu'il n'y ait pas de consensus sur le calendrier de développement de l'AGI, certains experts avancent des estimations très rapprochées, allant jusqu'à évoquer un horizon de 3 ans [[65], [66]](https://paperpile.com/c/V0trIA/ASqR+rC9t). Si ces projections s'avéraient exactes, cela pourrait impliquer une automatisation massive et soudaine de l'emploi. Même en l'absence de consensus sur ces questions, l'existence de telles préoccupations parmi une partie des experts justifierait que le rapport traite explicitement de ces scénarios potentiels.
* Elle néglige l'effet multiplicateur des futures avancées sur l'automatisation des tâches et des emplois. Une IA légèrement plus avancée pourrait non seulement effectuer des tâches existantes plus efficacement, mais aussi combiner ces tâches de manière nouvelle et automatiser des processus entiers. Ainsi, une petite avancée technologique pourrait déclencher une vague d'automatisation bien plus importante que prévu.

Cette approche non prospective rend toutes les conclusions du rapport excessivement optimistes, même si elles étaient par ailleurs justifiées si on refuse de considérer les progrès futurs à court terme.

**Manipulation rhétorique et sélection biaisée des données**

Non content d'admettre sans fondement l'hypothèse de l'arrêt des progrès en IA, le rapport emploie des techniques de manipulation rhétorique et une sélection biaisée des données pour présenter une vision artificiellement positive :



* Il affirme en introduction, en gras : "Notre propre analyse empirique suggère un effet positif de l'IA sur l'emploi" (p. 41). Cette conclusion est en contradiction avec les données présentées par la suite.
* En effet, la première approche, basée sur des études passées, présente des résultats mitigés et non concluants. Les auteurs eux-mêmes admettent que ces résultats "ne suffisent pas à appréhender l'ensemble des effets potentiels de l'IA sur le marché du travail" (p. 44) et soulignent le manque de recul temporel, particulièrement pour l'IA générative. Pourtant, de manière surprenante et sans justification, le rapport affirme ensuite : "Comme la précédente, cette approche conduit à prédire un effet positif de l'IA sur l'emploi." (p. 45). Cette conclusion est en contradiction flagrante avec la prudence et les nuances exprimées dans l'analyse des données.
* La seconde approche cite deux études aux résultats divergents : l'une suggère un potentiel de remplacement de 5,1% des emplois [[67]](https://paperpile.com/c/V0trIA/pPX4x), l'autre de 30% [[68]](https://paperpile.com/c/V0trIA/B5SYZ). Le rapport minimise cette disparité alarmante, en se focalisant uniquement sur les résultats de la première étude et conclut à un effet positif.
* Face à cette incertitude et ces risques potentiels, le rapport n'évoque à aucun moment la nécessité d'appliquer un principe de précaution.

L'analyse des effets de l'IA sur l'emploi présentée dans ce rapport est doublement biaisée : elle repose sur une hypothèse irréaliste d'un arrêt du progrès technologique, et même dans ce cadre favorable, elle manipule le langage et déforme les données pour présenter une conclusion injustifiée et excessivement optimiste. Cette approche pourrait conduire à une dangereuse sous-estimation des risques de disruption massive du marché du travail.


##### 3.3.2.2 Cybersécurité

Sections du rapport concernées: a, b, c

L'IA présente des défis nouveaux et significatifs en matière de cybersécurité, largement ignorés par le rapport de la Commission. De nombreuses études récentes [[57], [59], [60], [69]](https://paperpile.com/c/V0trIA/WhQyF+L5AsK+Q76Dv+AgWnR) mettent en évidence les risques de cybersécurité démultipliés par l'IA, ce qui aurait dû justifier un traitement approfondi de ces risques dans le rapport. Un bref état des lieux s'impose pour mesurer l'ampleur de ces risques.

**État des lieux des risques de cybersécurité liés à l'IA**



* Capacités d'attaque améliorées : L'IA permet le développement de techniques d'attaque plus sophistiquées, notamment de phishing personnalisé à grande échelle [[70]](https://paperpile.com/c/V0trIA/P5tbl), de deepfakes convaincants [[71]](https://paperpile.com/c/V0trIA/Fss8R), et de malwares adaptatifs.
* Surface d'attaque élargie : L'adoption généralisée des systèmes d'IA introduit de nouvelles vulnérabilités, comme les attaques adversariales [[72]](https://paperpile.com/c/V0trIA/aMgnc) ou l'extraction de données sensibles [[73]](https://paperpile.com/c/V0trIA/U4I99).
* Automatisation et évolution rapide des menaces : D'après le NCSC [[74]](https://paperpile.com/c/V0trIA/2S4qr), l'IA permet aux cybercriminels d'automatiser leurs opérations, notamment la reconnaissance, l'ingénierie sociale et le développement de logiciels malveillants, ce qui rend les attaques plus efficaces et plus rapides.
* Découverte et exploitation automatisées de vulnérabilités : Les systèmes d'IA avancés peuvent analyser le code, trouver des failles et les exploiter de manière autonome. GPT-4, par exemple, surpasse déjà 88% des pirates humains dans certaines compétitions [[60]](https://paperpile.com/c/V0trIA/AgWnR).
* Le NCSC prévoit avec une quasi-certitude une augmentation du volume et de l'impact des cyberattaques au cours des deux prochaines années.

Ces risques sont pris très au sérieux par les experts du domaine. 93% des professionnels en cybersécurité [[75]](https://paperpile.com/c/V0trIA/LpQ77) estiment qu'un "événement cyber catastrophique de grande ampleur est probable dans les deux prochaines années" et 97% [[76]](https://paperpile.com/c/V0trIA/4H8x2) estiment que leur organisation va subir un incident de sécurité causé par une IA.


![alt_text](images/image3.png "image_tooltip")


_Cette figure montre l'évolution projetée du coût annuel mondial de la cybercriminalité, exprimé en trillions de dollars américains. Les projections révèlent une croissance accélérée entre 2018 et 2023. Les coûts devraient passer de 0,86 trillion USD en 2018 à 13,82 trillions USD en 2028._

**Traitement de la cybersécurité dans le rapport**

Malgré l'importance de ces risques, et l'admission par le rapport de la Commission qu'ils existent, celui-ci ne leur accorde qu'une attention minimale et biaisée. Le mot "cybersécurité" n'apparaît que deux fois dans les 130 pages du rapport :



* "Rien n'indique toutefois que l'IA changera durablement le rapport de force entre cybercriminels et ceux chargés de nous protéger, à condition que ces derniers puissent s'emparer de ces technologies." (p. 32) \
Une étude de GovAI [[77]](https://paperpile.com/c/V0trIA/z6cbi) contredit cette affirmation, notant que "L'équilibre entre l'attaque et la défense risque de pencher davantage en faveur de l'attaque à mesure que les modèles fondamentaux deviennent de plus en plus performants.". Il existe une asymétrie fondamentale entre l’attaque et la défense en cybersécurité [[78], [79]](https://paperpile.com/c/V0trIA/pckBE+nABiG) : un défenseur doit sécuriser l'ensemble des points vulnérables de son infrastructure, tandis qu’un attaquant n’a besoin d’en exploiter qu'un seul pour réussir. À ce jour, aucun mécanisme ne permet d’utiliser l’IA pour corriger systématiquement toutes les failles potentielles à travers différents environnements, bien qu’il existe des spéculations sur le sujet selon lesquelles l’IA pourrait être utilisée pour automatiser la réaction aux attaques [[80]](https://paperpile.com/c/V0trIA/Zgy6).
* "Pour les risques biologiques et cyber, rien n'indique que les modèles ouverts posent plus de risques que des modèles fermés." (p. 59) \
Cette assertion n'est étayée par aucune preuve ou référence dans le rapport, notamment en ce qui concerne les risques cyber. Bien au contraire, tout indique que les modèles ouverts posent plus de problèmes que les modèles fermés, comme nous le détaillons dans la section "Open source". Alors que les modèles fermés peuvent encore être contrôlés par leurs développeurs pour prévenir des usages malveillants, les modèles open source échappent fondamentalement à tout contrôle, ce qui permet aujourd'hui déjà à des utilisateurs malintentionnés de les exploiter [[81]](https://paperpile.com/c/V0trIA/zYwzv).

Le rapport ignore ainsi complètement l'ampleur et la complexité des menaces posées par l'IA dans le domaine de la cybersécurité et faillit ainsi à sa mission d'informer adéquatement les décideurs et le public sur les défis critiques auxquels nous sommes confrontés.


##### 3.3.2.3 Open source

Sections du rapport concernées: a, b, c

L'open source permet le partage libre et collaboratif de code, favorisant l'innovation et l'accessibilité technologique. Bien que bénéfique dans de nombreux domaines, cette approche peut poser des risques graves lorsqu'elle est appliquée à des technologies dangereuses. Dans un contexte où les experts du domaine alertent sur des risques potentiellement catastrophiques à court terme, la question de l'open source des modèles d'IA les plus avancés devient un enjeu majeur de sécurité nationale et internationale.

Les dangers potentiels liés à l'open source des modèles d'IA avancés sont particulièrement préoccupants, comme le souligne une étude approfondie de GovAI [[77]](https://paperpile.com/c/V0trIA/z6cbi).



* **Capacités dangereuses et menaces pour la sécurité** : L'open sourcing de modèles hautement compétents pourrait entraîner la diffusion de capacités dangereuses, susceptibles de causer des dommages physiques importants ou de perturber des fonctions sociétales essentielles. Des acteurs malveillants pourraient exploiter ces modèles pour faciliter le développement d’armes biologiques et chimiques, lancer des cyberattaques contre des infrastructures critiques, ou encore faciliter la diffusion de désinformation à grande échelle et la surveillance coercitive de la population [[82]](https://paperpile.com/c/V0trIA/zUYC).
* **Désalignement rapide et perte de contrôle** : Une fois un modèle rendu public, il échappe rapidement au contrôle de ses créateurs, car il est rapidement modifié pour supprimer les restrictions de sécurité imposées. En moyenne, une version sans restrictions apparaît dans les quelques jours suivant la publication d'un modèle open source, rendant inefficace toute tentative de limiter ou contrôler son utilisation malveillante. Ce processus est accessible à tous et ne coûte qu'environ 200 dollars [[81]](https://paperpile.com/c/V0trIA/zYwzv).
* **Caractère irréversible de l'open sourcing** : Une fois un modèle rendu public, il devient impossible de l'éliminer ou de corriger des failles de sécurité découvertes ultérieurement, ce qui amplifie le risque d'exploitation malveillante de manière irréversible.

Face à ces préoccupations majeures, le traitement de la question de l'open source par la Commission de l'IA est alarmant par sa légèreté. Le rapport affirme d'emblée que l'ouverture des modèles d'IA ne pose "pas de risque supplémentaire significatif" (p.59), une conclusion en contradiction flagrante avec l'étude de GovAI. Cette position repose sur des hypothèses non justifiées et des omissions graves.



* La Commission ignore complètement les risques de cybersécurité liés à l'open source, se contentant d'affirmer leur inexistence sans aucune justification.
* Concernant les biorisques, le rapport se base sur une seule étude [[83]](https://paperpile.com/c/V0trIA/e8nXr) montrant l'absence de risque avec les modèles actuels, ignorant ainsi la possibilité de risques futurs. Comme nous l'avons montré plus haut, le rapport suppose implicitement un arrêt soudain des progrès en IA : voici un exemple clair montrant que cela donne lieu à une sous-estimation dangereuse des risques.
* Le rapport admet que les modèles open source réduisent de 70% le coût de production de la désinformation, mais il minimise ce risque en le considérant comme déjà inévitable. Il justifie cette position en affirmant que ces modèles sont déjà disponibles en open source, sans considérer que l'amélioration continue de ces technologies va certainement amplifier ce problème. 

La Commission traite l'IA comme une technologie statique, sans danger significatif, et en conclut que l'open source ne présente pas de dangers supplémentaires. Cette approche est d'autant plus choquante qu'elle ne serait certainement pas appliquée à d'autres technologies potentiellement dangereuses. On imagine mal, par exemple, des recommandations similaires pour rendre open source les plans de construction d'armes biologiques, nucléaires ou de virus informatiques.

Il est important de noter que cette analyse biaisée est certainement influencée par les conflits d'intérêts au sein de la Commission, de nombreux membres ayant des intérêts financiers directs dans le développement de l'IA open source que nous analysons en profondeur en section 5.


##### 3.3.2.4 Création artistique

Sections du rapport concernées: a, b, c

L'impact de l'IA sur la création artistique est un sujet de préoccupation croissante dans le monde de l'art et de la culture. Des mouvements d'artistes en colère, tels que ArtistsHate [[84]](https://paperpile.com/c/V0trIA/pv0Kb), émergent en réaction à la menace perçue de l'IA sur leurs métiers et leur créativité. Ces inquiétudes ne sont pas infondées, comme le montre une étude récente menée par CVL Economics [[85]](https://paperpile.com/c/V0trIA/sLQT5) sur l'impact de l'IA générative dans l'industrie du divertissement.

[Encart]

État des lieux : une disruption massive en cours

L'étude <span style={{textDecoration: "underline"}}>Future Unscripted</span> [[85]](https://paperpile.com/c/V0trIA/sLQT5) révèle l'ampleur de la transformation en cours :



* 72% des entreprises du divertissement sont des adopteurs précoces de l'IA générative, contre seulement 3,9% dans l'économie globale.
* 203 800 emplois américains dans le divertissement devraient être perturbés d'ici 2026, principalement par consolidation ou remplacement.
* L'industrie du film, de la télévision et de l'animation sera la plus touchée, avec 21,4% de sa main-d'œuvre impactée.
* Le secteur du jeu vidéo affiche le taux d'adoption le plus élevé, près de 90% des entreprises utilisant déjà l'IA générative.

Ces chiffres alarmants contrastent fortement avec le traitement superficiel et biaisé de la question par le rapport de la Commission de l'IA.

[Fin Encart] 

**Une analyse déficiente et trompeuse**

Le rapport de la Commission traite la question de l'impact de l'IA sur la création artistique de manière particulièrement problématique :



* Minimisation flagrante des impacts négatifs : Bien que le rapport reconnaisse brièvement les inquiétudes du secteur, il les balaie rapidement pour se concentrer sur les aspects positifs. Il n'y a aucune analyse approfondie des conséquences potentiellement dévastatrices pour certains métiers artistiques, en contradiction totale avec les données présentées dans l'étude de CVL Enonomics [[85]](https://paperpile.com/c/V0trIA/sLQT5).
* Absence totale de données et d'études : Alors qu’une étude de Goldman Sachs [[86]](https://paperpile.com/c/V0trIA/ECzq5) prévoit une automatisation de 26% des tâches dans le domaine artistique rien qu’avec les IA actuelles, le rapport de la Commission ne s'appuie sur aucune donnée chiffrée ni aucune étude pour soutenir ses affirmations. Cette absence de rigueur scientifique est particulièrement frappante et compromet sérieusement la crédibilité de l'analyse.
* Vision à court terme et hypothèse d'arrêt du progrès : Comme dans le reste du rapport (voir section 3.2.3), l'analyse de la Commission souffre d'une vision à court terme qui suppose implicitement un arrêt brusque des progrès de l'IA. Cette approche ignore complètement la rapidité des avancées dans le domaine et les projections à moyen terme. Le rapport affirme par exemple que "L'IA ne met cependant pas en danger l'originalité de la création en elle-même et ses processus de sélection" (p.53), sans aucune justification ni prise en compte des développements futurs potentiels de l'IA. Cette affirmation péremptoire contraste fortement avec les inquiétudes légitimes exprimées par de nombreux artistes.

En conclusion, le traitement de la question de l'impact de l'IA sur la création artistique par la Commission est symptomatique des problèmes qui entachent l'ensemble du rapport : minimisation des risques, manque de rigueur scientifique et vision à court terme. Cette approche faillit gravement à la mission d'éclairer les décideurs et le public sur les défis réels posés par l'IA dans le domaine artistique et culturel, et pourrait conduire à une dangereuse sous-estimation des bouleversements à venir dans ce secteur crucial de notre société.


#### 3.3.3 Manipulation linguistique

Sections du rapport concernées: a, b, c

Le rapport de la Commission de l'IA emploie diverses techniques de manipulation linguistique pour minimiser les risques potentiels liés à l'IA générative et façonner une perception rassurante mais trompeuse des enjeux. Ces techniques se manifestent à travers les choix lexicaux, les connotations émotionnelles, la discréditation des scénarios catastrophiques et l'utilisation d'amalgames et de fausses analogies.

Le choix des termes et des expressions tout au long du rapport révèle une volonté délibérée de désamorcer les inquiétudes et de favoriser la confiance du public. Le titre de la première partie, "Dédiaboliser l'IA, sans pour autant l'idéaliser", illustre parfaitement cette approche. Il suggère que les craintes liées à l'IA sont exagérées et doivent être tempérées, tout en prétendant adopter une position équilibrée.

La discréditation des scénarios critiques est particulièrement flagrante dans le traitement des avertissements émis par de nombreux scientifiques. Le rapport qualifie ces scénarios "d'épouvante", une expression chargée émotionnellement qui les assimilent à des fantasmes irrationnels plutôt qu'à des préoccupations légitimes basées sur des analyses scientifiques. Cette caractérisation contraste fortement avec les expressions plus neutres comme "effets indésirables" ou "actes malveillants" utilisées pour décrire les risques reconnus par la Commission. Cette disparité dans le choix des termes vise clairement à miner la crédibilité des préoccupations les plus graves.

Le rapport s'appuie sur une comparaison fallacieuse entre l'IA et l'électricité. Cette analogie, présentée sans justification approfondie, sert à banaliser les risques potentiels de l'IA en l'assimilant à une technologie familière et largement acceptée. Cependant, cette comparaison ignore les différences fondamentales entre ces deux technologies, notamment en termes de complexité, d'autonomie et d'impact potentiel sur la société. L'utilisation de cette fausse analogie révèle une tentative de simplification excessive des enjeux liés à l'IA, ce qui constitue une forme de manipulation argumentative.

Le rapport utilise également des amalgames et de fausses analogies pour banaliser les risques liés à l'IA. La formulation "Faut-il avoir peur de l'IA ? Non, mais il faut être vigilant comme avec tout outil" (p.31) est un exemple frappant de cette technique. En assimilant l'IA à un simple outil, le rapport ignore la nature fondamentalement différente et potentiellement autonome des systèmes d'IA avancés, notamment les risques liés aux systèmes d'IA désalignés.

La structure même du rapport contribue à cette manipulation linguistique. Chaque chapitre commence par un scénario positif, créant d'emblée un cadre optimiste. Bien que le développement qui suit nuance parfois ce tableau, l'impression initiale positive persiste, influençant la perception globale du lecteur. Cette technique est bien illustrée dans la partie traitant de la perte d'emploi. Alors que l'analyse présentée dans le corps du texte soulève des préoccupations sérieuses quant à l'impact de l'IA sur le marché du travail, le chapitre est encadré par des conclusions résolument optimistes. Il s'ouvre sur une vision positive des transformations à venir et se clôt sur des perspectives encourageantes, créant ainsi un effet de "sandwich optimiste" qui atténue la gravité des problèmes soulevés. Cette structure crée un décalage entre le contenu analytique, qui reconnaît des défis majeurs, et le message global véhiculé, qui minimise ces mêmes défis.

Cette approche linguistique révèle un biais d'optimisme prononcé et une volonté de traiter les enjeux de l'IA de manière superficielle. En minimisant systématiquement les risques à travers le langage utilisé, le rapport de la Commission échoue à fournir une analyse équilibrée et approfondie des défis posés par l'IA, compromettant ainsi sa crédibilité et son utilité en tant que document d'orientation stratégique.


### 3.4 Négligence des préoccupations citoyennes et approche non-démocratique


#### 3.4.1 Perception publique de l'IA : un mélange de préoccupations et d'attentes

Selon plusieurs sondages récents [[87], [88], [89]](https://paperpile.com/c/V0trIA/57LHE+wb9p2+KfPuW), les Français expriment des inquiétudes significatives concernant l'IA :



* 72% sont préoccupés ou ambivalents vis-à-vis de l'IA, le taux le plus élevé parmi 21 pays sondés.
* 78% s'inquiètent de la sécurité des données, de la vie privée et des droits d'auteur.
* 91% se sentent mal informés des risques et implications de l'IA.
* 68% sont favorables à l'établissement de règles contraignantes au niveau étatique.

Parallèlement, les Français reconnaissent le potentiel positif de l'IA dans certains domaines comme la santé et la recherche, mais expriment une forte méfiance quant à son utilisation pour des décisions critiques (justice, transport, diagnostic médical).


#### 3.4.2 Décalage entre les préoccupations publiques et l'approche de la Commission

Le rapport de la Commission reconnaît l'existence d'une perception négative de l'IA par le public français. Cependant, au lieu d'explorer en profondeur les raisons de ces inquiétudes et d'envisager une possible divergence fondamentale entre les aspirations du public et la direction actuelle du développement de l'IA, la Commission semble privilégier une approche paternaliste visant à modifier cette perception.

Le rapport mentionne bien l'importance du débat public :

"Nous recommandons de lancer immédiatement un plan de sensibilisation et de formation de la nation. Pour y parvenir, nous devons d'abord créer les conditions d'une appropriation collective de l'IA et de ses enjeux. Cela suppose d'animer en continu des débats publics dans notre société [...]" (p. 7)

Cependant, les propositions concrètes semblent davantage orientées vers la familiarisation et l'acceptation de l'IA que vers une consultation citoyenne sur les orientations à prendre :

"[...] susciter la création de lieux d'expérimentation et d'appropriation de la technologie (les « cafés IA »), de mettre à disposition un outil numérique d'information ou encore de lancer un concours de cas d'usages positifs de l'IA." (p. 7)

Cette approche, bien qu'elle mentionne le dialogue, semble privilégier une vision de communication unilatérale descendante, de l’expert vers le public, où l'objectif principal est de former et de convaincre le public plutôt que de l'impliquer véritablement dans les décisions concernant le développement et le déploiement de l'IA.

De plus, depuis la publication du rapport, l'accent semble avoir été mis principalement sur les "cafés IA", une initiative qui vise avant tout à réconcilier le public avec l'IA, plutôt que sur l'organisation de véritables débats publics permettant d'aborder les préoccupations citoyennes de manière approfondie. Il est d’ailleurs à noter que l’occultation et la minimisation des dangers par le rapport (voir sections 3.2.2 et 3.3) contribue à un débat public mort dans l'œuf, car les citoyens seraient privés des informations nécessaires pour prendre des décisions éclairées. 

Dans un contexte de grande incertitude sur les risques et bénéfices potentiels de l’IA, il convient de garder l’esprit ouvert et de consulter la population française sur les principes qui guideront la recherche et le développement de l’IA, afin de respecter ses priorités sans chercher à imposer une vision de l’IA avant qu’un consensus scientifique s’affermisse sur le sujet.

Cette approche soulève des questions sur la légitimité démocratique des décisions recommandées par le rapport. Les inquiétudes du public et les avertissements des experts reflètent des appréhensions légitimes face à cette technologie en rapide évolution. En l’absence de consensus scientifique, il convient d’informer tant sur les risques que sur les bénéfices potentiels, et par respect des opinions citoyennes, une gouvernance plus inclusive de l’IA doit prendre en compte ces préoccupations pour assurer un développement démocratique et éclairé de cette technologie.


#### 3.4.3 Proposition controversée sur l'accès aux données

Le rapport suggère de "faciliter l'accès aux données à caractère personnel" (p. 10) pour l'entraînement des modèles d'IA, par exemple en supprimant les procédures d’autorisation préalable à l’accès des données (p. 101). Cette recommandation va à l'encontre des préoccupations majeures du public concernant la protection de la vie privée, mais soulève également de sérieuses préoccupations éthiques et pratiques.

L'intégration de données personnelles dans les modèles de langage (_LLM_) présente des risques significatifs souvent sous-estimés :



* Les _LLM _ont tendance à mémoriser une grande partie des données sur lesquelles ils sont entraînés. Ces informations se retrouvent encodées dans les milliards de paramètres du modèle.
* Des techniques de "jailbreaking" peuvent potentiellement être utilisées pour extraire ces données personnelles du modèle. Cela signifie qu'un attaquant pourrait, en théorie, récupérer des informations privées qui n'auraient jamais dû être accessibles [[90]](https://paperpile.com/c/V0trIA/mtD8).
* L'utilisation de données personnelles pour l'entraînement de modèles d'IA soulève des questions importantes sur le consentement éclairé et la transparence envers les individus dont les données sont utilisées.

Le rapport ne mentionne aucun de ces risques propres à l’IA pour les données personnelles. Cette approche révèle une tendance inquiétante à privilégier le développement technologique au détriment des droits fondamentaux à la vie privée et à la protection des données personnelles. Elle nécessite un débat public approfondi et une évaluation rigoureuse des risques avant toute mise en œuvre.


### 3.5 Analyse comparative


#### 3.5.1 Introduction

L'essor spectaculaire des modèles de fondation en intelligence artificielle a incité de nombreux pays et organisations internationales à anticiper les bénéfices et les risques liés à leur développement et à leur déploiement.

En novembre 2023, le gouvernement britannique a lancé une initiative pionnière de coordination internationale à Bletchley, près de Londres [[91]](https://paperpile.com/c/V0trIA/fsuS). Ce sommet a réuni des représentants de nombreux gouvernements et organisations, aboutissant à une déclaration conjointe, signée par la France, soulignant l'importance de développer des IAs de manière sécurisée [[92]](https://paperpile.com/c/V0trIA/fq98).

Depuis lors, plusieurs pays et groupes de réflexion ont mandaté des experts pour dresser un état des lieux de l'intelligence artificielle et anticiper ses trajectoires de développement potentielles. Cette démarche est cruciale pour éclairer les décisions futures dans ce domaine.

Le rapport de la Commission française sur l'IA se voulait, en principe, aligné sur cette approche. Cependant, il se démarque par ses lacunes flagrantes comparé à d'autres initiatives similaires. Le document échoue à traiter la question avec le sérieux qu'elle mérite, éludant la plupart des risques sans les examiner en profondeur. Il ne sollicite aucun expert dans les domaines de risques les plus couramment anticipés, tels que les biorisques ou la sécurité informatique, et ne propose donc aucune solution politique ou institutionnelle crédible face à ces enjeux.

En négligeant d'aborder les risques liés à l'IA avec la rigueur nécessaire, le rapport se trouve en contradiction avec les résolutions co-signées par la France à Bletchley.

Des exemples de rapports plus complets et rigoureux existent pourtant. C'est pourquoi nous analysons ici les points forts d'autres initiatives comparables, dont les conclusions diffèrent nécessairement et significativement de celles du rapport de la Commission.

Pour évaluer la qualité du rapport de la Commission française sur l'IA, nous l'avons comparée à quatre initiatives récentes, reconnues pour leur rigueur et leur influence dans le domaine de la gouvernance de l'IA avancée. Ces documents représentent diverses approches, allant de l'analyse scientifique approfondie à la législation concrète, et couvrent des régions géopolitiques clés.



1. **Le _Gladstone Report _**[[93]](https://paperpile.com/c/V0trIA/i9pT)** **: Commandé par le Département d'État américain, ce rapport offre une évaluation approfondie des risques liés à l'IA avancée. Il se distingue par son analyse détaillée des risques catastrophiques et propose un plan d'action gouvernemental inédit, structuré autour de cinq axes d'effort.
2. **L’_International Scientific Report on the Safety of Advanced AI_ (_ISR_) **[[84]](https://paperpile.com/c/V0trIA/C2ny) : Cette initiative scientifique mondiale réunit des experts de 30 pays, de l'UE et de l'ONU. Ce rapport intérimaire se concentre sur les systèmes d'IA à usage général, comme les grands modèles de langage. Il examine en profondeur les capacités actuelles et futures de l'IA, les méthodologies d'évaluation, les risques potentiels et les approches techniques pour les atténuer.
3. **La loi européenne sur l’IA_ [[94]](https://paperpile.com/c/V0trIA/qEt2) _**: C'est la première réglementation complète sur l'intelligence artificielle proposée par un régulateur majeur. Initié par la Commission européenne, cet acte législatif vise à établir un cadre juridique pour le développement et l'utilisation de l'IA au sein de l'Union européenne. Il adopte une approche basée sur les risques, catégorisant les applications d'IA selon leur niveau de danger potentiel.
4. **L'_Executive Order on the Safe, Secure, and Trustworthy Development and Use of Artificial Intelligence_ (_Executive Order_) **américain sur l'IA [[95]](https://paperpile.com/c/V0trIA/wgii) : Signé par le président Joe Biden, ce décret exhaustif couvre un large éventail de domaines, de la sécurité nationale à la protection des consommateurs. Il établit des principes directeurs pour le développement responsable de l'IA, impose de nouvelles exigences aux entreprises développant des systèmes d'IA puissants, et mandate de nombreuses actions spécifiques des agences fédérales.

Les deux rapports (_Gladstone Report _et _ISR_) abordent en profondeur de nombreux risques fondamentaux. Ils font appel à de nombreux spécialistes et à une littérature abondante, évaluent les arguments pour et contre différents scénarios, et **expriment leurs conclusions en partageant leurs degrés d'incertitude**. Les deux textes législatifs (loi européenne sur l’IA_ _et _Executive Order_), quant à eux, font un effort d'anticipation des risques très important tout en cherchant à respecter la libre entreprise.

La comparaison de ces textes avec le rapport de la Commission française montre que ce dernier ne remplit pas son rôle d'information concernant les différents risques majeurs d'un développement mal maîtrisé de l'IA. Notre évaluation est résumée dans la figure suivante.


<table><tbody>
  <tr>
   <td>
<strong>Risque</strong>
   </td>
   <td>
<strong>Rapport français</strong>
   </td>
   <td>
<strong><em>Gladstone report</em></strong>
   </td>
   <td>
<strong><em>ISR</em></strong>
   </td>
   <td>
<strong>Loi européenne sur l’IA</strong>
   </td>
   <td>
<strong><em>Executive order</em></strong>
   </td>
  </tr>
  <tr>
   <td>
<strong>Cybersécurité</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Bioterrorisme</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Perte d'emploi</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Perte de controle</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Guerre informationelle</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Armes autonomes</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Violations de confidentialité</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Biais</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Deepfake</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Escalade des conflits internationaux</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
  <tr>
   <td>
<strong>Impact environnemental</strong>
   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
   <td>

   </td>
  </tr>
</tbody></table>



<table><tbody>
  <tr>
   <td>
<strong>Legende</strong>
   </td>
  </tr>
  <tr>
   <td>
Hors du champ d'application déclaré du rapport
   </td>
  </tr>
  <tr>
   <td>
Aucune mention ou mention très brève sans analyse
   </td>
  </tr>
  <tr>
   <td>
Mentionné avec une analyse superficielle, références peu nombreuses ou biaisées. Le rapport mentionne le risque mais l'analyse est limitée erronée ou trompeuse. Les références, si présentes, sont peu nombreuses ou sélectionnées pour soutenir un point de vue particulier.
   </td>
  </tr>
  <tr>
   <td>
Analyse modérée avec quelques références variées, mais potentiellement incomplète. Le rapport fournit une analyse plus substantielle avec quelques références variées. Cependant, certains aspects ou points de vue importants peuvent être sous-représentés.
   </td>
  </tr>
  <tr>
   <td>
Analyse approfondie avec des références solides, diverses et équilibrées, présentant différents points de vue. Le rapport offre une analyse complète, citant une variété de sources qui représentent différents points de vue sur le sujet. Il présente une image équilibrée des débats ou controverses existants.
   </td>
  </tr>
</tbody></table>



#### 3.5.2 Les risques éludés par le rapport français


##### 3.5.2.1 Perte de contrôle des modèles d'IA avancés

Le  _Gladstone report (p. 24 et 36-37) _et _l'ISR (section 4.2.3)_, examinent sérieusement la possibilité de perdre le contrôle d'IAs de niveau humain ou supérieur tandis que la loi européenne sur l’IA (chapitre 3) et l'_Executive Order_ américain proposent des mesures concrètes pour évaluer ces risques et contrôler le développement des modèles.

En revanche, le rapport de la Commission française se contente d'une phrase pour écarter ces préoccupations. Il utilise une longue analogie comparant l'IA à l'adoption de l'électricité, ce qui est trompeur car l'IA représente un changement bien plus fondamental (voir section 3.3.3).


##### 3.5.2.2 Biorisques et cybersécurité

Le _Gladstone report (p. 21) _et _l'ISR (section 4.1.4) _décrivent des scénarios où un modèle de fondation futur serait utilisé pour transmettre des informations critiques dans la fabrication d'armes biologiques, voire d'automatiser leur production. Les risques de facilitation des cyberattaques sont également reconnus par l'_ISR_, qui reste serein concernant les modèles publiés au moment de la rédaction du rapport (4.1.3). Le _Gladstone report_ prend ce risque au sérieux (ex. p. 24) et recommande la création d'institutions surveillant ces capacités, comme un Observatoire de l'IA (p. 51). La loi européenne sur l’IA classe ces modèles comme présentant des "risques systémiques" (_Recital_ 110) et exige des contrôles stricts. L'Executive Order reconnaît ces dangers et propose des mesures pour les évaluer et les atténuer.

Le rapport français affirme simplement que "rien n'indique" que les modèles actuels soient particulièrement dangereux à cet égard. Cette affirmation est non seulement fausse concernant les cyberattaques (voir section 3.3.2.2), mais elle ignore surtout la trajectoire rapide des progrès en IA.


##### 3.5.2.3 Deepfakes et désinformation

L'_ISR_ reconnaît les risques liés aux _deepfakes _et à la désinformation (p. 41-43), précisant qu'aucune technique robuste n'existe pour les prévenir (p. 76, 77, 82, 83). Le _Gladstone report _souligne l'inquiétude des chercheurs concernant la manipulation de l'opinion publique (p. 35, 43, Annexe F), et propose des systèmes de détection en amont (p. 273). La loi européenne sur l’IA interdit le déploiement de modèles d'IAs utilisant des techniques de désinformation et reconnaît les risques liés aux _deepfakes _(_Recitals_ 132 et 133).

Le rapport français sous-estime ces risques. Sa principale solution, la labellisation des contenus générés par IA, est insuffisante et techniquement inachevée (ISR p. 77).


##### 3.5.2.4 Confidentialité des données

L'_ISR_ souligne que les modèles de langage actuels sont peu sécurisés et peuvent divulguer des informations confidentielles (p. 60-61), précisant qu'aucune technique n'est connue pour empêcher totalement ces fuites (p. 81-82). La loi européenne sur l’IA ordonne aux développeurs de rendre leurs modèles plus sûrs (Article 15).

Le rapport français ne traite pas sérieusement cette question. Au contraire, il suggère d'adapter la CNIL pour permettre une utilisation plus large des données d'utilisateurs, sans reconnaître l'absence de solution technique pour garantir leur confidentialité (voir section 3.4.3).


##### 3.5.2.5 Autres risques non traités

Le rapport français omet plusieurs risques importants :



* L'escalade des conflits internationaux due à une course à l'armement en IA (_Gladstone report_ p. 19, 85)
* La création et la mise à disposition du public de capacités de création d'armes autonomes (_Gladstone report_ p. 84, _ISR _p.12, 16)
* Les biais discriminatoires que les modèles peuvent intégrer (_ISR_ p. 49-51, loi européenne sur l’IA, _Recital_ 67, _Executive Order_)

Le _Gladstone report _préconise la mise en place d'une Agence Internationale de l'IA et un contrôle international sur la chaîne d'approvisionnement globale de l'IA (p.19). La loi européenne sur l’IA_ _(chapitre III et IX)_ _et l'_Executive Order_ recommandent la mise en place de régulations et de systèmes de vérification des données d'entraînement et du comportement des modèles.

Cette comparaison montre que le rapport français ne traite pas de manière adéquate de nombreux risques potentiels liés au développement de l'IA, contrairement aux initiatives internationales qui adoptent une approche plus complète et prudente.


#### 3.5.3 Les risques traités superficiellement

Le rapport de la Commission française n'aborde en profondeur que deux catégories de risques, non catastrophiques. Malgré leur importance, le traitement reste insuffisant comparé aux autres rapports internationaux.


##### 3.5.3.1 Pertes d'emplois

Le rapport consacre 7 pages à la question de la perte d'emploi, en faisant le risque le plus discuté. Cependant, comme discuté en détails dans la section 3.3.2.1, l'analyse présente plusieurs faiblesses :



* Les données présentées montrent que peu d'études ont été réalisées, limitées à quelques secteurs d'activité.
* Les résultats sont mitigés et ne suggèrent pas un effet positif évident de l'IA sur l'emploi.
* Les études sont basées sur des données passées, ne prenant pas en compte l'évolution rapide des modèles d'IA.
* Le rapport n'étudie pas l'effet économique d'une potentielle automatisation à bas coût d'une grande partie du travail humain.

En contraste, l'_ISR_ adopte une approche plus prudente, notant l'incertitude et l'absence de consensus parmi les économistes sur ce sujet (p.54-56). Il présente une vision plus riche des différentes difficultés liées à cette situation inédite.

L'_Executive Order_ américain va plus loin en demandant à son administration de préparer un rapport spécifique sur les conséquences de l'IA sur le marché du travail, et de proposer des solutions aux risques identifiés.


##### 
    3.5.3.2 Impact environnemental

Malgré un effort réel d'estimation, le rapport de la Commission minimise l'impact environnemental de l'IA. Puisque les autres risques ne sont pas pris au sérieux, l'impact de l'IA est considéré comme nécessairement positif, et donc peut être favorablement mis en regard de son coût environnemental.

En revanche, l'_ISR_ consacre deux pages à une discussion plus approfondie et arrive à une conclusion radicalement différente (p. 59-60) :



* Malgré les incitations à rendre les modèles plus économiques en énergie, l'augmentation de la demande excède largement les améliorations dans l'efficacité des entraînements.
* Les entraînements des modèles d'IA augmentent très rapidement en taille et en nombre, ce qui accroît leur impact environnemental.


#### 3.5.4 Conclusion

L'incapacité de la Commission à envisager des scénarios crédibles et pluriels pour le développement de l'IA se traduit logiquement par une absence de propositions concrètes pour faire face aux défis potentiels.

La mission confiée par le gouvernement à la Commission est cruciale et survient à un moment très important de notre histoire. Le manque de rigueur dans la production du rapport est flagrant et préoccupant. On note l'emploi récurrent d'une rhétorique de la moquerie pour qualifier les tenants d'une posture prudente et systématique.

L'approche de la Commission contraste fortement avec les résolutions co-signées par la France lors du sommet de Bletchley, ce qui pourrait affecter la crédibilité internationale de la France sur les questions d'IA. Cela réduit également nos chances d'anticiper et de prévenir les risques les plus importants liés au développement stupéfiant de l'intelligence artificielle.
</ReportSection>

<ReportSection id="analyse-de-la-composition-de-la-commission-de-lia" number="4" navTitle="Analyse de la composition de la Commission de l’IA">
Dans un contexte où l'IA soulève des enjeux sociétaux majeurs, l'impartialité des instances guidant les politiques publiques est cruciale. Comme nous l'avons montré dans la partie précédente, le rapport de la Commission de l'IA présente des omissions sur les risques potentiels et un enthousiasme marqué pour l'open source. Ces positions coïncident de manière troublante avec les intérêts de certaines entreprises représentées au sein de la Commission, notamment Meta et Mistral, ce qui soulève des questions légitimes sur la composition de la Commission et son influence sur les recommandations produites.

Cette section vise à mettre en lumière les dysfonctionnements dans la composition de la Commission qui pourraient expliquer les lacunes du rapport. Nous examinerons le manque de diversité, les conflits d'intérêts majeurs, et les actions controversées de certains membres, soulignant l'importance d'une composition plus neutre pour des recommandations plus équilibrées.

**Points clés à retenir :**



* Une surreprésentation significative de l'industrie par rapport aux experts en éthique et sécurité de l'IA.
* Des conflits d'intérêts majeurs chez plusieurs membres influents, liés à des géants technologiques et start-ups d'IA.
* Des actions de lobbying et des prises de position controversées remettant en question l'impartialité du processus.
* Ces éléments fournissent une explication potentielle aux lacunes identifiées dans le rapport de la Commission.


### 4.1 Manque de diversité

La composition d'une Commission chargée d'élaborer des recommandations sur l'IA influence directement la qualité et l'exhaustivité de ses conclusions. La présence d'experts en éthique et en sécurité de l'IA est particulièrement importante, car ces disciplines se penchent spécifiquement sur les impacts sociétaux et les risques liés à cette technologie.

Pour analyser la composition de la Commission, nous avons choisi de nous concentrer sur des axes qui nous semblaient particulièrement pertinents : la représentation de l'industrie par rapport au monde académique, et la présence d'experts en éthique et en sécurité de l'IA. Cette catégorisation s'est basée sur les affiliations principales de chaque membre au moment de la Commission. Une description détaillée de la méthodologie est disponible en annexe.


![alt_text](images/image4.png "image_tooltip")


Ce graphique illustre clairement la surreprésentation du secteur privé par rapport au monde académique, ainsi que l'absence d'experts en sécurité de l'IA et la sous-représentation des spécialistes en éthique de l'IA.

[Encart]

**Ethique et sécurité de l’IA**

Bien que la distinction entre éthique et sécurité de l'IA soit parfois contestée, elle est utile pour notre analyse. Nous définissons l'éthique de l'IA comme la discipline examinant les implications morales et sociétales de l'IA, en établissant des principes pour son développement et son utilisation responsables, tandis que la sécurité de l’IA vise à développer des systèmes d'intelligence artificielle sûrs et fiables, en minimisant les risques de dommages non intentionnels.

[Fin de l’encart]

Cette composition déséquilibrée a des implications directes sur les lacunes du rapport :

1. L'absence totale d'experts en sécurité de l'IA explique en partie l'omission des risques les plus sévères et le manque d'anticipation des progrès futurs.

2. La sous-représentation des experts en éthique contribue à la minimisation des risques d’utilisations malveillantes.

3. La surreprésentation de l'industrie, dont les intérêts penchent vers moins de régulation, renforce ces tendances.

Dans un entretien [[97]](https://paperpile.com/c/V0trIA/LiGM), Anne Bouverot, co-présidente de la Commission, justifie la forte présence de l'industrie par le fait que la majorité de l'innovation en IA provient du secteur privé. Cependant, cet argument ne tient pas compte de la nature transversale de l'IA et de ses impacts sociétaux qui dépassent largement le cadre de l'innovation technologique.

Nous déplorons également le manque de transparence de la Commission concernant les experts consultés durant le processus. Aucun des experts en sécurité de l'IA que nous avons nous-mêmes consultés n'avait été approché par la Commission.

Pour une Commission plus équilibrée et à même de produire des recommandations tenant compte de tous les enjeux, nous aurions souhaité voir au minimum une parité entre représentants de l'industrie et du monde académique, avec au moins la moitié des membres représentant l'éthique et la sécurité de l'IA pour contrebalancer les intérêts privés.

L'absence de certains types d'experts (spécialistes de l'alignement, chercheurs en sécurité de l'IA, psychologues, sociologues du travail) est particulièrement préoccupante. Bien que la présence de figures éminentes de l'industrie comme Yann LeCun soit compréhensible, il est essentiel d'assurer un équilibre des perspectives. Par analogie, une Commission sur l'énergie incluant des représentants de l'industrie pétrolière devrait également intégrer des experts en environnement et en énergies renouvelables. De même, une Commission sur l'IA devrait inclure des experts en sécurité et en éthique de l'IA pour contrebalancer les perspectives de l'industrie.


### 4.2 Conflits d’intérêts

Les conflits d'intérêts au sein d'une Commission chargée de guider les politiques publiques sont toujours préoccupants, mais ils prennent une dimension particulière dans le domaine de l'intelligence artificielle. Un conflit d'intérêt survient lorsqu'un individu est en position d'exploiter sa capacité professionnelle ou officielle d'une manière qui pourrait bénéficier à ses intérêts personnels ou corporatifs au détriment de l'intérêt général.

L'IA, en tant que technologie transversale, impacte un large éventail de secteurs industriels, ce qui signifie qu'une grande partie des représentants de l'industrie pourraient être considérés comme ayant un certain degré de conflit d'intérêt. La surreprésentation de l'industrie au sein de la Commission, démontrée dans la section précédente, accentue ce risque et souligne l'importance d'intégrer des perspectives diverses pour équilibrer les enjeux de l'IA.

Notre analyse se concentre sur les conflits les plus prononcés, impliquant les représentants des entreprises créatrices de modèles d'IA, qui seraient les plus directement impactés par d'éventuelles régulations. Nous avons également considéré les efforts de lobbying anti-régulation, notamment de Meta, Microsoft, Google et Mistral, pour identifier les cas où les intérêts personnels ou corporatifs pourraient influencer les recommandations de la Commission.

Activités de lobbying des entreprises concernées :



1. Meta (anciennement Facebook) déploie des efforts massifs et une stratégie coordonnée pour influencer la régulation de l'IA. En 2023, l'entreprise a investi 14,6 millions de dollars en lobbying direct auprès du Congrès américain et de l'administration Biden sur l'IA [[98]](https://paperpile.com/c/V0trIA/m5C2C). Meta finance l'American Edge Project, un groupe de pression qualifié de "chien d'attaque anti-réglementaire de Facebook"[[99]](https://paperpile.com/c/V0trIA/TyPM), qui a reçu 85,5 millions de dollars entre 2019 et 2022. Ce groupe a dépensé plus de 150 000 dollars en publicités Facebook contre la régulation de l'IA entre février et juin 2024 [[100]](https://paperpile.com/c/V0trIA/CW0Vf).
2. Google investit des ressources considérables dans le lobbying contre la régulation de l'IA. En 2023, l'entreprise a dépensé 9,2 millions de dollars en lobbying auprès des législateurs américains sur des questions liées à l'IA et à la propriété intellectuelle [[98]](https://paperpile.com/c/V0trIA/m5C2C). L'entreprise soutient que la doctrine du "_fair use_" protège l'IA des infractions au droit d'auteur. Google s'oppose activement à la proposition de loi californienne SB 1047 visant à réguler l'IA [[101]](https://paperpile.com/c/V0trIA/HAX6j), tout comme d'autres géants technologiques. 
3. Mistral AI a joué un rôle crucial dans le lobbying contre la loi européenne sur l’IA [[102]](https://paperpile.com/c/V0trIA/SJguQ). L'entreprise a rapidement obtenu un accès privilégié aux plus hauts niveaux de décision, notamment grâce à Cédric O, son principal lobbyiste et membre de la Commission de l'IA. Ancien secrétaire d'État au Numérique, O a activement fait pression pour diluer les exigences sur les développeurs d'IA à usage général. Ce lobbying intensif a significativement influencé la position française sur l'AI Act. (Pour plus de détails sur ce sujet, voir la section [4.3.1])

Sur cette base, nous avons identifié les personnes suivantes pour une analyse approfondie de leurs potentiels conflits d'intérêts : Yann LeCun (Meta), Arthur Mensch (Mistral), Cédric O (Mistral), Joelle Barral (Google)

Dans les sections suivantes, nous examinerons en détail les conflits d'intérêts potentiels de ces membres de la Commission, en analysant comment leur affiliation à ces entreprises pourrait influencer leurs positions et recommandations.

_Une liste complète des activités professionnelles de tous les membres de la Commission est disponible en annexe._


#### 4.2.1 Yann LeCun

[https://www.linkedin.com/in/yann-lecun/](https://www.linkedin.com/in/yann-lecun/)



* Vice President & scientifique en chef de l'IA chez Meta. Anciennement Facebook.
* Cofondateur, conseiller a Element Inc. Entreprise R&D de reconnaissance biométrique qui s'associe à des institutions financières, des gouvernements et des organismes de santé pour transformer la manière dont ils fournissent des services.

Yann LeCun est une figure emblématique dans le domaine de l'intelligence artificielle. Lauréat du prestigieux prix Turing en 2018, souvent considéré comme un équivalent du prix Nobel en informatique, il est également membre de l'Académie des sciences française [[103]](https://paperpile.com/c/V0trIA/hLcuz) et Chevalier de la Légion d'honneur [[104]](https://paperpile.com/c/V0trIA/HeJKh). Ces distinctions témoignent de l'importance de ses contributions scientifiques et de sa reconnaissance au plus haut niveau de la communauté scientifique et de l'État français.

L'influence de LeCun s'étend bien au-delà du monde académique. Avec plus de 800 000 _followers _sur Twitter [[105]](https://paperpile.com/c/V0trIA/KG7rp) et 782 000 sur LinkedIn [[106]](https://paperpile.com/c/V0trIA/PzRyk), il est une voix prédominante dans le débat public sur l'IA. Sa présence médiatique est considérable, avec des centaines d'apparitions et mentions dans les médias en 2023. Cette influence a été reconnue par le magazine TIME, qui l'a inclus dans sa liste "Time 100 AI" [[107]](https://paperpile.com/c/V0trIA/4WMYq) des personnes les plus influentes dans le monde de l'IA. Son rayonnement atteint les sphères politiques et économiques les plus élevées, comme en témoignent ses rencontres avec le Président Emmanuel Macron [[108]](https://paperpile.com/c/V0trIA/8FO1b) et sa participation au Forum économique mondial de Davos [[109]](https://paperpile.com/c/V0trIA/xA2H1). Cette stature exceptionnelle dans les milieux scientifiques, médiatiques et politiques suggère que les opinions de LeCun pourraient avoir un poids significatif au sein de la Commission.

Malgré son expertise incontestable, la présence de Yann LeCun au sein de la Commission soulève des questions importantes quant aux conflits d'intérêts potentiels. En effet, LeCun occupe actuellement le poste de Vice-Président et Scientifique en Chef de l'IA chez Meta, une entreprise qui a été au cœur de nombreux scandales éthiques majeurs ces dernières années (voir encadré "L'éthique contestable de Meta"). Il décrit lui-même son rôle comme étant "focalisé sur la direction scientifique, la stratégie IA, et l'évangélisation externe" [[110]](https://paperpile.com/c/V0trIA/eP6AK). Or, Meta développe activement des modèles d'IA à la frontière technologique, précisément le type de modèles dont le développement rapide est considéré comme potentiellement risqué par de nombreux experts. De plus, Meta promeut l'approche open source pour ses modèles d'IA, une stratégie qui soulève des inquiétudes en termes de sécurité et de contrôle (voir section 3.3.2.3). LeCun est également cofondateur et conseiller d'Element Inc., une entreprise spécialisée dans la reconnaissance biométrique, ce qui ajoute une couche supplémentaire de conflit potentiel, notamment sur les questions de protection de la vie privée et d'éthique liées à ces technologies.

Il est également important de noter que LeCun est connu pour ses opinions controversées sur les risques liés à l'IA et pour sa tendance à recourir à des attaques _ad hominem_ envers ses opposants. Il a notamment qualifié de membres d'une "secte apocalyptique"[[111]](https://paperpile.com/c/V0trIA/HB33) ceux qui ne partagent pas sa vision optimiste des dangers de l'IA, y compris ses co-lauréats du prix Turing, Yoshua Bengio et Geoffrey Hinton. Cette attitude soulève des questions quant à sa capacité à considérer objectivement les risques potentiels de l'IA dans le cadre de son rôle au sein de la Commission.

[Debut Encart]

**L'éthique contestable de Meta**

Meta (anciennement Facebook) a été au cœur de plusieurs scandales éthiques majeurs :



1. Le scandale Cambridge Analytica (2018) [(Confessore 2018)](https://paperpile.com/c/V0trIA/R8Ij) a révélé l'exploitation massive de données personnelles de millions d'utilisateurs à des fins de manipulation politique.
2. Les "Facebook Files" (2021) [(The Facebook Files )](https://paperpile.com/c/V0trIA/11hK), une enquête du Wall Street Journal basée sur des documents internes, ont exposé de nombreuses pratiques problématiques de l'entreprise, notamment :
    1. L'amplification algorithmique de contenus toxiques et de désinformation.
    2. L'aggravation des problèmes de santé mentale, particulièrement chez les adolescents.
    3. Le laxisme face au trafic de drogue, au trafic d'êtres humains et aux activités mafieuses sur ses plateformes.
3. Amnesty International a accusé Meta d'avoir contribué au génocide des Rohingyas en Birmanie [[112]](https://paperpile.com/c/V0trIA/xkf8) et à la violence ethnique en Éthiopie [[113]](https://paperpile.com/c/V0trIA/Xvvf), en amplifiant massivement des appels à la violence sur ses plateformes.

Ces révélations suggèrent que Meta a souvent privilégié la croissance et les profits au détriment de la sécurité des utilisateurs [[114]](https://paperpile.com/c/V0trIA/39kA). Dans le contexte de l'IA, Meta, disposant d'immenses bases de données et de ressources informatiques, a un intérêt évident à limiter la régulation. 

Yann LeCun, occupant un poste de direction stratégique durant ces scandales, a systématiquement défendu l'entreprise [[114], [115], [116]](https://paperpile.com/c/V0trIA/0tav+M0XB+39kA), soulevant des questions sur sa capacité à évaluer objectivement les risques éthiques de l'IA au sein de la Commission.

[Fin Encart]


#### 4.2.2 Arthur Mensch

[https://www.linkedin.com/in/arthur-mensch/](https://www.linkedin.com/in/arthur-mensch/)



* Cofondateur et PDG de Mistral AI

Arthur Mensch est cofondateur et PDG de Mistral AI [[117]](https://paperpile.com/c/V0trIA/BMoE), une start-up française devenue en peu de temps un leader dans le développement de modèles d'intelligence artificielle générative, notamment open source. En juin 2024, Mistral AI était valorisée à 6 milliards d'euros [[118]](https://paperpile.com/c/V0trIA/6L6k8), la positionnant comme une "pépite" française face aux géants technologiques américains.

Le conflit d'intérêts de Mensch au sein de la Commission de l'IA est évident :



* En tant que PDG, Mensch a un intérêt direct à minimiser la régulation de l'IA pour favoriser le développement rapide de son entreprise.
* Mistral AI, sous la direction de Mensch, a activement fait pression contre certaines dispositions de la loi européenne sur l’IA, notamment celles concernant la régulation des modèles de fondation (voir section 4.3.3).
* Lors d'une audition au Sénat, Mensch a tenu des propos trompeurs minimisant les risques liés à l'IA et remettant en question la nécessité d'une régulation stricte (voir section 4.3.3).
* Le récent accord avec Microsoft, un acteur majeur de l'IA, soulève des questions sur l'indépendance de Mistral et ses positions futures en matière de régulation.

La présence de Mensch dans la Commission, combinée à ses intérêts commerciaux directs et ses positions publiques, crée un risque significatif que les recommandations de la Commission soient influencées en faveur d'une régulation minimale de l'IA, potentiellement au détriment de considérations éthiques et de sécurité plus larges.


#### 4.2.3 Cedric O

[https://www.linkedin.com/in/cedric-o/](https://www.linkedin.com/in/cedric-o/)



* Conseiller cofondateur de Mistral AI
* Membre du conseil d'administration d'Artefact

Cédric O, ancien secrétaire d'État au Numérique, présente des conflits d'intérêts significatifs :



1. Rôle chez Mistral AI : Conseiller cofondateur et principal lobbyiste de Mistral AI. Selon Capital [[119]](https://paperpile.com/c/V0trIA/7QeEk), O aurait investi dans Mistral AI via son agence de conseil, un investissement valorisé à 23 millions d'euros en décembre 2023.
2. Changement radical de position : Après avoir défendu des réglementations strictes en tant que secrétaire d'État, O plaide désormais pour une dérégulation de l'IA, alignant ses positions sur les intérêts de Mistral AI.
3. O a été au cœur d'un scandale de lobbying contre la loi européenne sur l’IA (voir section 4.3.1).
4. Membre du conseil d'administration d'Artefact, une société de conseil dédiée à l'accélération de l'adoption de l'IA, renforçant son intérêt pour une régulation minimale.

La gravité de ces conflits d'intérêts est soulignée par la décision de la Haute Autorité pour la Transparence de la Vie Publique, **qui a interdit à O, pour une durée de trois ans, de faire du lobbying auprès du gouvernement** ou de détenir des actions dans des entreprises technologiques [(de la Vie Publique 2022)](https://paperpile.com/c/V0trIA/SrcL)_._


#### 4.2.4 Joelle Barral

[https://www.linkedin.com/in/joellebarral/?originalSubdomain=fr](https://www.linkedin.com/in/joellebarral/?originalSubdomain=fr)



* Directeur principal de la recherche et de l'ingénierie chez Google DeepMind
* Directeur de l'ingénierie chez Google

Google DeepMind, filiale de Google, est l'un des leaders mondiaux dans le développement d'IA avancées, ayant notamment créé des modèles avancés emblématiques comme AlphaGo et AlphaFold.

Les positions de Barral soulèvent des questions de conflits d'intérêts potentiels. En tant que représentante de Google et DeepMind, elle pourrait être incitée à favoriser des politiques bénéficiant à ces entreprises. Étant donné les efforts de lobbying de Google contre la régulation de l'IA, sa présence dans la Commission pourrait influencer les recommandations dans un sens favorable à l'industrie. Son double rôle pourrait ainsi compromettre son impartialité dans l'évaluation des risques et bénéfices de l'IA avancée.


### 4.3 Actions et opinions controversées

Certains membres de la Commission ont été impliqués dans des actions ou ont exprimé des opinions qui soulèvent des questions quant à leur impartialité et leur capacité à évaluer objectivement les enjeux liés à l'IA. Ces controverses, détaillées ci-dessous, mettent en lumière des positions potentiellement biaisées en faveur d'un développement peu régulé de l'IA, au détriment d'une approche plus prudente prenant en compte les risques potentiels.

Les actions et opinions controversées que nous avons retenues incluent :



1. **Lobbying de Cédric O : **L'ancien secrétaire d'État au Numérique, devenu conseiller de Mistral AI, a été accusé de conflit d'intérêt dans son opposition à la loi européenne sur l’IA.
2. **Lettre au Sénat Américain :** Yann LeCun et Arthur Mensch ont signé une lettre faisant la promotion de l'open source et contenant des affirmations scientifiquement erronées.
3. **Déclarations d'Arthur Mensch devant le Sénat :** Lors d'une audition, le cofondateur de Mistral AI a tenu des propos trompeurs sur le contrôle des modèles d'IA et la régulation du secteur.


#### 4.3.1 Lobbying de Cédric O


##### 4.3.1.1 Contexte

Cédric O, ancien secrétaire d'État au Numérique et membre de la Commission de l'IA, a joué un rôle clé dans l'élaboration de réglementations européennes rigoureuses comme le Digital Services Act et le Digital Markets Act. Après son mandat, il est devenu conseiller cofondateur et principal lobbyiste de Mistral AI, une start-up française d'intelligence artificielle.


##### 4.3.1.2 Actions controversées

Depuis son arrivée chez Mistral AI, O a radicalement changé de position, plaidant pour une dérégulation de l'intelligence artificielle. En octobre 2023, il affirme [[120]](https://paperpile.com/c/V0trIA/ghIWL) que la loi européenne sur l’IA pourrait "tuer" son entreprise. Selon Médiapart, il aurait réussi à influencer le gouvernement pour qu'il adopte cette nouvelle approche [[121], [122], [123]](https://paperpile.com/c/V0trIA/fzErG+62cK6+JV9a8). Il est considéré comme l'architecte principal de l'opposition française à la loi européenne sur l’IA [[124], [125], [126]](https://paperpile.com/c/V0trIA/C8c3P+G086D+GKV6c).

O a notamment organisé une lettre ouverte [[127]](https://paperpile.com/c/V0trIA/MEVyu) signée par plus de 150 dirigeants d'entreprises, dont plusieurs membres de la Commission de l'IA, mettant en garde contre une réglementation trop stricte des modèles de fondation. Avec l'appui de l'Allemagne et de l'Italie, la France s'est opposée  à toute règle contraignante pour les fournisseurs de modèles de fondation [[128]](https://paperpile.com/c/V0trIA/5Hkbm).

Mme Morin-Desailly, vice-présidente de la Commission des Affaires Européennes, a déclaré : "O n'a pas déclaré Mistral auprès de la Haute Autorité pour la Transparence de la Vie Publique, qui l'avait déjà mis en garde de ne pas se faire embaucher par Atos ou toutes autres sociétés technologiques."[[126]](https://paperpile.com/c/V0trIA/GKV6c)


##### 4.3.1.3 Réactions et critiques

Ce revirement a suscité de vives critiques. Le député Philippe Latombe a exprimé ses inquiétudes : "Le fait que la Haute Autorité ait demandé à Cédric O de déclarer tous ses prises de participations dans le secteur de la tech soulève des questions sur son actionnariat au sein de Mistral et, par conséquent, au sein du Comité pour l'intelligence artificielle générative."[[126]](https://paperpile.com/c/V0trIA/GKV6c)

Le commissaire européen Thierry Breton a également critiqué cette position, déclarant que Cédric O défendait "tout sauf l'intérêt général" [[129]](https://paperpile.com/c/V0trIA/lKYza).

Face à ces accusations, O s'est défendu dans une réponse publique en affirmant : "Je n'ai pas changé de position depuis mes précédentes fonctions" [[130]](https://paperpile.com/c/V0trIA/vBuDf). Cependant, la Haute Autorité pour la Transparence de la Vie Publique lui a interdit [(de la Vie Publique 2022)](https://paperpile.com/c/V0trIA/SrcL), pour trois ans, de faire du lobbying auprès du gouvernement ou de détenir des actions dans des entreprises technologiques.


##### 4.3.1.4 Implications

Ces controverses soulèvent des questions importantes sur l'impartialité de la Commission de l'IA. Le double rôle de Cédric O, à la fois membre de la Commission et lobbyiste pour une entreprise d'IA, ainsi que son investissement personnel dans Mistral AI (valorisé à 23 millions d'euros en décembre 2023 [[119]](https://paperpile.com/c/V0trIA/7QeEk)), créent un conflit d'intérêt très prononcé.


#### 4.3.2 Lettre au Sénat Américain


##### 4.3.2.1 Contexte

Yann LeCun et Arthur Mensch, membres de la Commission de l'IA, ont co-signé une lettre adressée au Sénat américain et à la Chambre des Lords britannique [[131]](https://paperpile.com/c/V0trIA/WP89S), faisant la promotion de l'open source dans le développement de l'IA.


##### 4.3.2.2 Affirmation mensongère ou incompétence

La lettre contient une déclaration scientifiquement erronée :

"_Although advocates for AI safety guidelines often allude to the "black box" nature of AI models, where the logic behind their conclusions is not transparent, recent advancements in the AI sector have resolved this issue, thereby ensuring the integrity of open-source code models_."

(“Bien que des partisans de recommandations en sécurité de l’IA fassent souvent allusion à la nature “boîte noire” des modèles en IA, où la logique derrière leurs conclusions n’est pas transparente, des avancées récentes dans le secteur de l’IA ont résolu ce problème, garantissant ainsi l’intégrité des modèles de code en open source.”)

Cette affirmation est en contradiction directe avec le consensus scientifique actuel [(Hassija et al. 2024; Chaszczewicz 2023; Barredo Arrieta et al. 2020)](https://paperpile.com/c/V0trIA/aOeZD+BjUe+HUFm). Bien que notre capacité à analyser certains aspects des modèles d'IA s'améliore, il est incorrect de dire que la nature de "boîte noire" de l'IA a été résolue, même partiellement. Les réseaux de neurones profonds, notamment les modèles de langage, restent opaques, même avec un code source ouvert. Ces modèles, comportant souvent des milliards de paramètres, présentent des interactions complexes non interprétables par les experts. Les outils d'interprétabilité, comme les cadres d'IA explicable, ont permis des avancées, mais nous n'en sommes qu'aux balbutiements de cette discipline. De plus, les récents progrès en IA, en rendant les modèles plus grands et plus complexes, exacerbent ce problème.

Cette déclaration erronée est utilisée pour soutenir l'idée que les modèles open source sont sans danger, une position qui favorise les intérêts des entreprises impliquées.


##### 4.3.2.3 Réactions et critiques

De nombreux chercheurs et experts en interprétabilité ont vivement critiqué cette affirmation [[133]](https://paperpile.com/c/V0trIA/YHZpB), voici quelques exemples:



* Neel Nanda (Google Deepmind) [[134]](https://paperpile.com/c/V0trIA/s0mh7) : "_WTF?! This is massively against the scientific consensus_." (“Pardon ?! C’est absolument contraire au consensus scientifique !”)
* Max Kesin (Meta) : “_it's either deep stupidity/ignorance or more likely a blatant lie in service of their thesis_” (“c’est soit de la stupidité/de l’ignorance, soit plus probablement un mensonge éhonté pour servir leur argumentaire”)
* Darren McKee : “_No. And how odd._” (“Faux. Et que c’est bizarre [de dire ça].”)

Martin Casado, l'auteur de la lettre, s'est excusé pour cette déclaration, admettant qu'elle était incorrecte [[135]](https://paperpile.com/c/V0trIA/rRolZ). John Carmack, un des signataires, a reconnu  que l'affirmation était "clairement incorrecte" [[136]](https://paperpile.com/c/V0trIA/2Nmax).

Cependant, Arthur Mensch et Yann LeCun n'ont pas présenté de réponse ou de rétractation.


##### 4.3.2.4 Implications

Le fait que ces membres influents de la Commission aient signé une lettre adressée à des instances législatives majeures contenant des affirmations scientifiquement erronées, sans les corriger par la suite, remet en question :



* La rigueur scientifique des membres de la Commission de l'IA
* Leur compréhension des enjeux actuels de l'IA
* Leur capacité à fournir des recommandations éclairées et impartiales


#### 4.3.3 Arthur Mensch devant le Sénat


##### 4.3.3.1 Contexte

Le 14 juin 2024, Arthur Mensch, cofondateur et directeur général de Mistral AI, a été auditionné par la Commission des affaires économiques du Sénat, présidée par Mme Dominique Estrosi Sassone. Cette audition visait à mieux comprendre les implications de l'IA pour l'écosystème français et les enjeux de régulation.


##### 4.3.3.2 Déclarations controversées

Au cours de cette audition, Mensch a fait plusieurs déclarations qui soulèvent des questions quant à leur exactitude scientifique et leur honnêteté intellectuelle [[137]](https://paperpile.com/c/V0trIA/bJmQ) :



1. "On parle de logiciel, y a pas de changement, c'est un langage de programmation, personne ne se fait contrôler par son langage de programmation".
2. "Il y a rien d'autonome, c'est un logiciel. Quand on écrit ce genre de logiciel, on contrôle toujours ce qu'il va passer et l'ensemble des sorties que le logiciel peut avoir. Il n'y a pas de changement de paradigme".
3. "On donne les outils pour que les développeurs contrôlent bien les applications qu'ils déploient".


##### 4.3.3.3 Analyse critique

Ces déclarations sont trompeuses pour plusieurs raisons (voir aussi section 3.3.1) :



1. Assimilation erronée à un langage de programmation : Les modèles d'IA, en particulier les grands modèles de langage (LLMs), ne sont pas des langages de programmation et ne fonctionnent pas comme des logiciels traditionnels. Ils génèrent des réponses basées sur des probabilités calculées à partir de vastes ensembles de données, ce qui les rend fondamentalement différents des logiciels classiques. Par exemple, les LLMs sont capables de persuader des humains [[138]](https://paperpile.com/c/V0trIA/ZlKN). Les langages de programmation ne le sont pas. 
2. Illusion de contrôle total : Contrairement à ce qu'affirme Mensch, les créateurs de modèles d'IA ne contrôlent pas directement chaque sortie possible. La complexité rend impossible la prédiction exacte de toutes les sorties sans exécuter le modèle [[139]](https://paperpile.com/c/V0trIA/pd9L). De plus, étant donné que les sorties de ces modèles sont du langage naturel humain, leur contrôle automatique est extrêmement complexe [(Hassija et al. 2024; Chaszczewicz 2023; Barredo Arrieta et al. 2020)](https://paperpile.com/c/V0trIA/aOeZD+BjUe+HUFm). 
3. Surestimation des outils de contrôle : Les méthodes proposées par Mistral [[140]](https://paperpile.com/c/V0trIA/hibG) pour contrôler les applications, basées sur quelques lignes d’instructions ajoutées au prompt, ne fournissent aucune garantie solide [[141]](https://paperpile.com/c/V0trIA/NsqZ). Même les laboratoires les plus avancés peinent à assurer l'intégrité éthique de leurs modèles [[142], [143], [144]](https://paperpile.com/c/V0trIA/8PKu+z6cl+K6s1).


##### 4.3.3.4 Implications

Ces déclarations minimisent la complexité et les défis posés par l'IA moderne. Elles présentent une vision simpliste qui pourrait induire en erreur les législateurs sur la nature réelle des technologies d'IA et les risques associés. Cette approche pourrait compromettre l'élaboration de réglementations appropriées et efficaces.

Le fait qu'un membre influent de la Commission de l'IA tienne de tels propos devant une instance législative majeure soulève des inquiétudes quant à la qualité des conseils fournis aux décideurs politiques et remet en question la capacité de la Commission à guider efficacement la politique française en matière d'IA.


### 4.4 Conclusion

L'analyse approfondie de la composition de la Commission de l'IA a mis en lumière plusieurs problèmes préoccupants :



1. Un déséquilibre marqué dans la représentation, avec une prédominance du secteur privé et une absence d'experts en sécurité de l'IA.
2. Des conflits d'intérêts significatifs chez des membres clés, notamment liés à des entreprises comme Meta et Mistral AI, qui ont un intérêt direct dans une régulation minimale de l'IA.
3. Des actions et déclarations controversées de certains membres, soulevant des questions sur leur objectivité et leur rigueur scientifique.

Ces défaillances dans la composition de la Commission ont des implications profondes. Elles remettent en question la capacité de cet organe à produire des recommandations véritablement équilibrées et objectives, prenant en compte l'ensemble des enjeux liés à l'IA, y compris les risques potentiels nécessitant si avérés des réactions de la part de certaines entreprises. 

La crédibilité du rapport final de la Commission s'en trouve sérieusement compromise. Les lacunes identifiées dans sa composition expliquent en grande partie les biais et les omissions observés dans le contenu du rapport, notamment la minimisation systématique des risques et l'absence de considération pour les enjeux de sécurité de l'IA.

Cette situation souligne la nécessité urgente de repenser la manière dont sont constituées les commissions chargées de guider les politiques publiques sur des sujets aussi cruciaux et complexes que l'IA. Sans une réforme significative de ce processus, il est à craindre que les recommandations produites ne servent pas pleinement l'intérêt général et ne préparent pas adéquatement la France aux défis futurs de l'IA.
</ReportSection>

<ReportSection id="recommandations" number="5" navTitle="Recommandations">
Notre analyse a mis en évidence des lacunes graves dans le rapport de la Commission de l'IA, compromettant potentiellement la stratégie de la France en matière d'intelligence artificielle. Ces défaillances semblent découler d'un manque de diversité d'expertise et de conflits d'intérêts au sein de la Commission. Pour remédier à cette situation et assurer une approche plus équilibrée et rigoureuse, nous proposons les recommandations suivantes :

**1. Remaniement de la Commission de l'IA**

   Éliminer les conflits d'intérêts et diversifier l'expertise au sein de la Commission. Écarter les membres ayant des conflits d'intérêts majeurs pour les repositionner comme consultants. Intégrer davantage de représentants de l'académie, de la société civile, de l'éthique et de la sécurité de l'IA.

**2. Consultation d'experts en sécurité de l'IA**

   Solliciter l'avis de spécialistes reconnus en sécurité de l'IA pour évaluer rigoureusement les risques liés au développement de systèmes d'IA avancés. Leur expertise est cruciale pour identifier les vulnérabilités et proposer des mesures de protection robustes. Il existe de tels spécialistes en France. Nous recommandons le Centre pour la Sécurité de l’IA (CeSIA), que nous avons consulté pour corriger des parties de ce document, SaferAI, ou encore le prix Turing Québécois Yoshua Bengio.

**3. Rédaction d'un addendum au rapport**

   Suite à la consultation d'experts, produire un addendum traitant spécifiquement des risques majeurs négligés dans le rapport initial. Cet ajout est essentiel pour offrir une vision complète des enjeux liés à l'IA et assurer la prise en compte de tous les aspects critiques dans l'élaboration des politiques futures.

**4. Organisation d'un débat public sur l'avenir de l'IA en France**

   Lancer une convention citoyenne pour informer le public sur les bénéfices potentiels et les risques de l'IA. L'objectif est d'évaluer l'acceptabilité sociale du développement de l'IA et d'enrichir la réflexion collective en prenant en compte les préoccupations de la société civile.

La mise en œuvre de ces recommandations devrait permettre d'établir une stratégie nationale en matière d'IA qui soit à la fois ambitieuse et responsable, tenant compte des opportunités mais aussi des risques potentiels de cette technologie transformative, en remédiant aux manquements présents du rapport que nous avons identifiés.
</ReportSection>

<ReportSection id="conclusion" number="6" navTitle="Conclusion">
L'analyse approfondie du rapport de la Commission de l'IA révèle des lacunes alarmantes qui compromettent gravement sa crédibilité et son utilité en tant que document d'orientation stratégique pour la France. Les omissions critiques concernant les risques existentiels et la sécurité de l'IA, la minimisation systématique des dangers potentiels, et le manque flagrant d'anticipation des développements futurs témoignent d'une approche dangereusement biaisée et myope.

Ces défaillances trouvent leur origine dans la composition même de la Commission, marquée par des conflits d'intérêts majeurs et un manque criant de diversité d'expertise, notamment l'absence totale de spécialistes en sécurité de l'IA. Cette structure déséquilibrée a conduit à un rapport qui privilégie manifestement les intérêts de l'industrie au détriment d'une évaluation objective et complète des enjeux.

Les implications de ces manquements sont potentiellement catastrophiques pour la stratégie française en matière d'IA. En sous-estimant systématiquement les risques et en négligeant les scénarios futurs plausibles, le rapport ouvre la voie à des réglementations inadéquates et à une préparation insuffisante face aux défis imminents posés par l'IA avancée.

Il est impératif d'adopter une approche plus équilibrée et rigoureuse, intégrant une diversité de perspectives et accordant une attention particulière aux questions de sécurité et d'éthique. Seule une évaluation objective et exhaustive des risques et des opportunités permettra d'élaborer une stratégie nationale à la hauteur des enjeux.

Face à cette situation critique, nous appelons à :



1. Un remaniement immédiat de la Commission, éliminant les conflits d'intérêts et intégrant une véritable diversité d'expertise.
2. La consultation d'experts en sécurité de l'IA.
3. La rédaction d'un addendum au rapport, traitant spécifiquement des risques majeurs précédemment négligés.
4. L'organisation d'un débat public national sur l'avenir de l'IA en France.

L'enjeu dépasse largement le cadre national : il s'agit de l'avenir de notre société et, potentiellement, de l'humanité elle-même. Le sommet sur l'IA prévu en 2025 offre à la France une opportunité unique de prendre une position de leadership sur ces questions cruciales. Il est temps pour notre pays de prendre la pleine mesure des défis et des dangers que pose cette technologie révolutionnaire, et d'agir en conséquence avec lucidité, responsabilité et ambition.
</ReportSection>

<ReportSection id="validation-des-experts" navTitle="Validation des experts">
Cette section présente les experts qui ont validé notre analyse, ainsi que leur niveau de soutien. **Les recommandations n'engagent que les auteurs principaux et n'ont pas fait l'objet d'une validation par les experts.**

**Définition d'expert et domaines d'expertise :**

Dans le cadre de cette contre-expertise, nous considérons comme "expert" toute personne possédant une expertise reconnue dans un ou plusieurs domaines pertinents pour l'analyse de l'IA et de ses impacts. Ces domaines incluent, sans s'y limiter : l'intelligence artificielle, l'éthique de l'IA, la sécurité de l'IA, la cybersécurité, l'informatique, l'apprentissage automatique, la politique publique, l'économie de l'innovation, le droit du numérique, la sociologie du travail, la psychologie cognitive, la philosophie des sciences, la bioéthique, l'environnement, les relations internationales, la santé publique, les sciences de l'information et la gestion des risques.

Les experts ont eu la possibilité de valider spécifiquement les sections relevant de leur domaine d'expertise. Cette approche permet une validation ciblée et précise, reflétant la nature interdisciplinaire des enjeux liés à l'IA.

**Niveaux de soutien :**



1. **Validation : **L'expert confirme l'exactitude et la pertinence de l'analyse présentée pour l'ensemble du document ou pour des sections spécifiques.
2. **Soutien :** L'expert approuve les principales conclusions de l'analyse, tout en pouvant avoir des réserves sur certains points spécifiques, pour l'ensemble du document ou pour des sections spécifiques.

**Liste des experts et leur niveau de soutien :**

[Nom de l'expert 1]

Titre, Affiliation

Domaine(s) d'expertise : [Liste des domaines pertinents]

Niveau de participation : [Validation / Contribution consultative]

Sections concernées : [Ensemble du document / Liste des sections spécifiques]

Commentaire : [Bref commentaire de l'expert, si fourni]
</ReportSection>

<ReportSection id="soutien-des-associations" navTitle="Soutien des associations">
Cette section présente les associations qui soutiennent notre contre-expertise. Le soutien d'une association n'implique pas nécessairement l'accord de tous ses membres avec chaque détail du rapport, mais indique un alignement général avec ses conclusions principales.

**Types de soutien :**



* Soutien global : L'association approuve l'ensemble de l'analyse et des conclusions principales du rapport.
* Soutien partiel : L'association soutient certaines parties spécifiques du rapport, qui sont explicitement mentionnées.

Liste des associations soutenant le rapport :

[Nom de l'association 1]

Type de soutien : [Soutien global / Soutien partiel]

Parties soutenues : [Si soutien partiel, liste des sections spécifiques]

Bref commentaire : [Optionnel, 1-2 phrases maximum]
</ReportSection>

<ReportSection id="a-propos-de-pause-ia" navTitle="A propos de Pause IA">
Pause IA est une association dédiée à la promotion d'un développement responsable et éthique de l'intelligence artificielle. Cette contre-expertise a été réalisée sous l'égide de Pause IA, reflétant l'engagement de l'association pour un débat public éclairé sur les enjeux de l'IA.

[ Quelques lignes sur la mission, les objectifs et les valeurs de Pause IA ]

Pour plus d'informations sur Pause IA : https://www.pauseia.fr
</ReportSection>

<ReportSection id="a-propos-des-auteurs" navTitle="A propos des auteurs">
Cette contre-expertise est le fruit d'un travail collectif bénévole, réalisé par un groupe diversifié d'experts et de citoyens engagés. Nos contributeurs partagent une préoccupation commune pour les enjeux de l'IA et un désir de promouvoir un débat public éclairé sur ce sujet crucial.

**Auteurs principaux :**

Maxime Fournes

Président, Pause IA

[Brève description de l'expertise pertinente, 1-2 phrases]

Pierre Lamotte

[Titre/Profession]

[Brève description de l'expertise pertinente, 1-2 phrases]

Éloïse Benito-Rodriguez

[Titre/Profession]

[Brève description de l'expertise pertinente, 1-2 phrases]

Gilles Breda

Olivier Lemasle

**Contributeurs :**

[Liste des noms des contributeurs qui ont apporté une aide significative mais qui ne sont pas considérés comme auteurs principaux]

Amaury Lorin

Aurelia Jauffret

**Direction du projet :**

Maxime Fournes

Nous tenons à souligner que tous les auteurs et contributeurs ont participé à ce projet sur leur temps personnel, sans aucune rémunération. Ce travail représente un effort collectif d'environ 300 heures.

**Conflits d'intérêts :**

Les auteurs déclarent n'avoir aucun conflit d'intérêt en relation avec le sujet de cette contre-expertise. Aucun financement externe n'a été reçu pour la réalisation de ce travail.

Pour toute question ou commentaire concernant ce document, veuillez contacter : [maxime@pauseia.fr](mailto:maxime@pauseia.fr) ou [gilles@pauseia.fr](mailto:gilles@pauseia.fr).
</ReportSection>

<ReportSection id="annexes" navTitle="Annexes">
### A - Graphique Composition Commission

Se réfère au graphique section 4.1.

Dans la catégorie “Industrie”, nous avons mis les entrepreneurs et les chercheurs dans le domaine privé. Cela inclut :

Gilles Babinet (entrepreneur), Anne Bouverot (entrepreneuse), Bernard Charlès (pdg de Dassault), Cédric O (lobbyiste), Arthur Mensch (entrepreneur et chercheur dans le privé), Yann Lecun (chercheur dans le privé), Cyprien Canivenc (directeur stratégie dans le privé), Joëlle Barral (chercheuse dans le privé), Luc Julia (chercheur dans le privé). Nozha Boujemaa (chercheuse dans le privé).

Dans la catégorie “Académie”, nous avons mis les chercheurs dans le domaine public, cela inclut:

Isabelle Ryl (chercheuse en IA), Gaël Varoquaux (chercheur en IA), Philippe Aghion (économiste), Alexandra Bensamoun (chercheuse en droit).

L’Éthique de l’IA examine les implications morales et sociétales de l'IA, en établissant des principes pour son développement et son utilisation responsables. Nous avons mis dans cette catégorie: \
Martin Tisné (PDG de AI Collaborative), Anne Bouverot (cofondatrice de la Fondation Abeona), Franca Salis-Madinier (étudie l’impact de l’IA sur le travail).

La sécurité de l’IA vise à développer des systèmes d'intelligence artificielle sûrs et fiables, en minimisant les risques de dommages intentionnels ou non. Personne dans la commission ne travaille dans ce domaine.


### B - Postes des membres de la Commission

**Anne Bouverot, co-présidente de la Commission**

[https://www.linkedin.com/in/anne-bouverot-0873a41/details/experience/](https://www.linkedin.com/in/anne-bouverot-0873a41/details/experience/)



* Présidente du conseil d'administration de Cellnex Telecom. Le premier opérateur européen d'infrastructures de télécommunications sans fil.
* Membre du conseil d'administration de Thomson Reuters. Agence de presse canado-britannique et société d'édition professionnelle, financière et juridique.
* Conseillère principale à TowerBrook Capital Partners L.P. Un fond d'investissement.
* Membre indépendante du conseil d'administration de Ledger. Licorne à croissance rapide qui développe des solutions de sécurité et d'infrastructure pour les crypto-monnaies et les applications blockchain.
* Présidente du Conseil d'administration de Technicolor Creative Studios de juin 2019 à février 2024. Un des principaux fournisseurs d'effets visuels et de services d'animation.
* Présidente du conseil d'administration de l’ENS.

**Philippe Aghion, co-président de la Commission**



* Professeur au collège de France.
* Professeur à l'INSEAD, école privée de management.
* Conseiller spécial transformation au Secrétariat général pour l'investissement.
* Membre du Conseil d'administration à l'Institut national du service public.

**Luc Julia**

[https://www.linkedin.com/in/lucjulia/](https://www.linkedin.com/in/lucjulia/)



* Directeur scientifique chez Renault.
* Cofondateur de ODIA, entreprise de synthèse vocale par intelligence artificielle.

**Gilles Babinet**

[https://www.linkedin.com/in/gillesbabinet](https://www.linkedin.com/in/gillesbabinet) 

[https://laitao.fr/consulting](https://laitao.fr/consulting)



* Entrepreneur. Propose des services de consulting aux entreprises souhaitant engager un processus de transformation numérique via sa plateforme Laitao.
* Professeur à HEC.
* Co-président du Conseil National du Numérique.
* Digital Champion pour la France à la Commission Européenne.
* Professeur Associé à Sciences Po.

**Bernard Charles**

[https://www.linkedin.com/in/dassaultsystemesceo/?originalSubdomain=fr](https://www.linkedin.com/in/dassaultsystemesceo/?originalSubdomain=fr)



* PDG de Dassault Systèmes, un éditeur de logiciels.
* Membre du conseil d'administration de Sanofi, entreprise pharmaceutique.

**Franca Salis- Madinier**

[https://www.linkedin.com/in/franca-madinier-3631b940/?originalSubdomain=fr](https://www.linkedin.com/in/franca-madinier-3631b940/?originalSubdomain=fr)



* Secrétaire nationale CFDT Cadres.
* Vice-présidente "workers group" au Comité Économique et Social Européen.
* Salariée d’Orange.

**Christophe Ravier**

[https://www.linkedin.com/in/christophe-ravier-9752a84/?originalSubdomain=fr](https://www.linkedin.com/in/christophe-ravier-9752a84/?originalSubdomain=fr)



* Directeur R&D BU Agro chez AKANEA, éditeur de logiciel.

**Nozha Boujemaa**

[https://www.linkedin.com/in/nozha-boujemaa-5471ab/?originalSubdomain=fr](https://www.linkedin.com/in/nozha-boujemaa-5471ab/?originalSubdomain=fr)



* Vice-présidente global - Innovation en matière d'IA et confiance chez Decathlon.
* Directrice des données industrielles à l'Adra - AI-Data-Robotics-Association. Partenaire privé du Partenariat européen ADR, qui cherche à stimuler la compétitivité européenne et le développement des synergies entre l’IA, la data et la robotique.
* Coprésidente du groupe d’experts OECD.AI.

**Gaël Varoquaux**

[https://www.linkedin.com/in/gael-varoquaux-a8391411/?originalSubdomain=fr](https://www.linkedin.com/in/gael-varoquaux-a8391411/?originalSubdomain=fr)



* Cofondateur de Therapixel.
* Cofondateur et conseiller scientifique de “:probabl”.
* Directeur de recherche à Inria.
* Cofondateur de scikit-learn.

**Philippe Chantepie**

[https://www.linkedin.com/in/philippe-chantepie-3178b42/?originalSubdomain=fr](https://www.linkedin.com/in/philippe-chantepie-3178b42/?originalSubdomain=fr)



* Inspecteur général au ministère de la Culture et Communication.
* Chercheur associé à la Chaire Innovation & Régulation de l’École Polytechnique / Telecom Paris Tech / Orange.

**Isabelle Ryl**

[https://www.linkedin.com/in/isabelle-ryl-82a799b/?originalSubdomain=fr](https://www.linkedin.com/in/isabelle-ryl-82a799b/?originalSubdomain=fr)



* Vice-présidente pour l'IA - Directrice du PRAIRIE (Institut de recherche en IA de Paris).
* Membre du Conseil d'administration de l'École des Ponts ParisTech.
* Membre du conseil d'administration - Trésorière a Agoranov.

**Alexandra Bensamoun**

[https://www.linkedin.com/in/alexandra-bensamoun-b3964298/?originalSubdomain=fr](https://www.linkedin.com/in/alexandra-bensamoun-b3964298/?originalSubdomain=fr)



* Chercheuse associée à l'université Laval.
* Professeure de droit à l'université Paris-Saclay.
* Experte internationale IP/IT sur la diversité des expressions culturelles à l'UNESCO.
* Membre du comité exécutif à l'Institut DATAIA.

**Martin Tisné**

[https://www.linkedin.com/in/martin-tisn%C3%A9-b7448122/?originalSubdomain=uk](https://www.linkedin.com/in/martin-tisn%C3%A9-b7448122/?originalSubdomain=uk)



* Pdg de AI Collaborative.
* Membre du conseil d’administration de Partnership on AI.
* Vice President de Luminate.

**Yann LeCun**

Voir section 4.2.1

**Cedric O**

Voir section 4.2.3

**Arthur Mensch**

Voir section 4.2.2

**Joëlle Barral \
**Voir section 4.2.4


### C - L'apprentissage profond et ses origines

Afin de corriger l’image présentée par le rapport d’une technologie mature, dont le fonctionnement et les implications sont compris depuis longtemps, et qui se place dans la stricte continuité des paradigmes précédents, rappelons l’histoire de l’IA moderne[^1].

Un changement de paradigme commence à s’opérer dans l'histoire de l'IA au milieu des années 1990 avec l’adoption du paradigme connexionnistme[^2], apparu dans les années 1970 et 1980 (Russel & Norvig 2020:25, Nilsson 2009:424). Les approches symboliques en IA, qui dominaient le champ jusqu'alors, sont remplacées par des méthodes permettant aux systèmes de s'adapter à la complexité des données issues du monde réel sans nécessiter la programmation de règles préétablies (Russel & Norvig 2020:24, Benzon 2023).

Ce paradigme comprend différentes architectures de réseaux neuronaux, les réseaux bayésiens, et d'autres méthodes statistiques comme les HMM[^3]. Ils sont capables d’analyser de vastes ensembles de données pour identifier des régularités, mais dépendent encore de l'ajustement manuel de leurs paramètres et de la qualité des données. Souvent de taille modeste, ils sont réputés pour leur transparence et leur interprétabilité, permettant une compréhension claire des processus décisionnels qu'ils emploient ([Kaminskaitè 2023](https://www.inveniam.fr/blog-brief-history-70-years-of-machine-learning)).

Les réseaux neuronaux apparaissent encore jusqu'à la fin des années 2000 comme une voie parmi d'autres , même s'ils prennent une importance grandissante dans la recherche en IA (Nilsson 2009, Russel & Norvig 2020). C’est véritablement avec l'apprentissage profond que ce paradigme prend son plein essor,  montrant une efficacité remarquable.La démonstration d'AlexNet en particulier, un modèle de reconnaissance d’images qui, en 2012, remporte la compétition internationale ImageNet, fait date (Russel & Norvig 2022:25, Benzon 2023).

Les réussites de l’apprentissage profond se multiplient par la suite, avec la création de modèles spécialisés dans toutes sortes de domaines, notamment dans des jeux de plus en plus complexes, ce qui est devenu apparent avec la victoire écrasante d’AlphaGo, un modèle de Google Deepmind contre Lee Sedol, un des meilleurs joueurs mondiaux de go en 2016 (https://deepmind.google/technologies/alphago/). En 2017, il bat le champion du monde Ke Jie (https://www.wikiwand.com/en/articles/AlphaGo_versus_Ke_Jie). On parle désormais de « révolution du _deep learning_ » ([Sejnowski](https://mitpress.mit.edu/author/terrence-j-sejnowski-2310) 2018, Russel & Norvig 2020:1.4).

Ce sont cependant les avancées dans la génération de texte par des grands modèles de langage (_LLM_) qui constituent le tournant majeur des modèles généralistes comme ChatGPT.


### D - Les _LLM_

Les _LLM _(_Large Language Model _- Grand modèle de langage)_ _sont entraînés à prédire l'élément suivant dans un texte en fonction d’un contexte, utilisant des estimations probabilistes pour générer du contenu cohérent. Cet entraînement produit une matrice vectorielle gigantesque, contenant toutes les relations observées entre les tokens (séries de caractères) sur lesquels le modèle est entraîné. Ces tokens, générés par un autre modèle d’intelligence artificielle, peuvent être des mots, des portions de mots ou des phrases.

Les modèles d’apprentissage profond sont d’autant plus efficaces qu’ils sont massifs, ce qui augmente également leur complexité et leur opacité. Les algorithmes, s’ils existent, menant d’une entrée (un _prompt_) à un résultat (un texte) sont invisibles aux « programmeurs » de l’IA, tout comme ils le sont pour l’IA elle-même. En réalité, un modèle d’apprentissage profond n’est pas « programmé » au sens traditionnel en informatique : ce que l’on programme, c’est sa structure et son algorithme d’apprentissage. Le modèle est ensuite entraîné et découvre des moyens de traiter l’information reçue  par lui-même, sous l’effet des contraintes et des données fournies. En ce sens, ces modèles sont moins « programmés » que « cultivés ».

Cette opacité est telle qu’elle a conduit à la création d’un nouveau champ des sciences informatiques, l’interprétabilité, destiné à sonder et comprendre le contenu des modèles.

Les _LLM_ sont particuliers car ils sont entraînés sur le langage humain, qui contient de nombreuses connaissances et modélisations du monde. Cela leur permet de développer des capacités de compréhension générale bien plus étendues que les modèles spécialisés en classification d’images ou reconnaissance d’objets. Cette compréhension du monde, et le fait qu’il soit possible de spécialiser des _LLM _par « réglage fin » (_fine tuning_) sur des données plus restreintes, conduit à les utiliser comme « modèles de fondation » à la base de toutes sortes d’applications spécialisées.

Les _LLM_ ont montré des capacités émergentes remarquables au fil des générations, comme la programmation de code informatique, la compréhension et la production de textes longs, la traduction multilingue, et l'explication de concepts complexes dans divers domaines. Cependant, certains comportements potentiellement dangereux ont été observés, comme le mensonge, la manipulation, et le hacking.

Ces capacités sont souvent découvertes après la mise à disposition des modèles au grand public. En plus de leurs capacités intrinsèques, il est possible d’améliorer l’efficacité des modèles de fondation grâce au _scaffolding_ ou « enrobage », qui consiste à les intégrer dans des programmes informatiques permettant l’exécution de fonctions, ou à les faire réfléchir par étapes. Les améliorations possibles sont difficiles à prédire.

Les IA actuelles sont en rupture totale avec les paradigmes précédents : opaques, cultivées plus que programmées, étonnantes dans leurs réussites comme dans leurs erreurs, elles se rapprochent plus que jamais de l’objectif du champ de recherche en IA, qui est de créer des machines qui pensent. Le paradigme, et les capacités qui l’accompagnent, sont tout jeunes – à peine plus d’une décennie pour l’apprentissage profond, la moitié d’une décennie pour les _LLM_. Les capacités de ces modèles de langage s'étendent rapidement, et les expériences du passé éclairent peu sur les capacités futures. La trajectoire du développement de ces modèles nous mène vers des risques d’augmentation brusque et incontrôlée de leurs capacités.


### E - La trajectoire actuelle de l’IA

Le premier facteur influençant la trajectoire actuelle de l'IA est la possibilité d'anticiper l'amélioration des modèles grâce aux  « _scaling laws_ » qui stipulent que l’efficacité des modèles de langage augmente logarithmiquement en fonction de leur taille, c’est-à-dire de la quantité de données et de calcul utilisée pour leur entraînement. Plusieurs fois dans leur courte histoire, des chercheurs ont prédit la fin de ces lois de la scalabilité, qui ont toutefois tenu bon jusqu'ici, malgré plusieurs ordres de grandeur d'augmentation de la taille des modèles. Les entreprises développant ces modèles parient sur la continuité de ces lois, cherchant à obtenir plus de données et de capacités de calcul, par exemple en générant artificiellement des données avec d’autres IA et en construisant de nombreux « _GPU clusters_ », à l’instar de Microsoft, qui projette même la construction d’une centrale nucléaire pour leur alimentation en électricité.

Des efforts considérables sont consacrés à la création de nouvelles puces et au développement de processeurs plus puissants. Par exemple, Sam Altman, PDG d’OpenAI, a récemment annoncé un projet de plusieurs milliers de milliards de dollars avec les Émirats Arabes Unis pour la construction de nouvelles puces destinées à l’entraînement des modèles.

Cette course à la puissance de calcul est désormais internationale et prend une tournure géopolitique. Les capacités de calcul actuelles sont déjà probablement supérieures à celles strictement nécessaires à la création de modèles bien plus intelligents. Ce phénomène, appelé  \
« _compute overhang_ », désigne une situation où une amélioration des modèles pourrait survenir de manière brusque avec la découverte d’algorithmes plus efficaces, démultipliant soudainement la valeur des capacités de calcul déjà existantes.

Les jeux de données utilisés pour l'entraînement connaissent également des transformations profondes. La qualité des données joue un rôle crucial dans la performance du modèle, et des efforts importants sont faits pour leur curation. L'innovation la plus notable des années 2023 et 2024 est la multimodalité des modèles, entraînés sur des images, des vidéos et du son en plus du texte. Les premiers modèles nativement multimodaux sont en développement, et ils pourraient combiner l'efficacité des modèles spécialisés avec la généralité des _LLM_.

Le _scaffolding _des modèles multimodaux sera un élément clé des systèmes destinés à améliorer leurs compétences. Enfin, les paradigmes précédents de l’histoire l'IA ne sont pas oubliés et un effort important est fourni pour tenter de les appliquer aux modèles actuels, produisant des phénomènes de "cross-pollinisation" pouvant déboucher rapidement sur la création de superintelligences.

La prochaine étape de l’histoire de l’IA est celle des agents autonomes. L’essentiel de la valeur ajoutée de l’intelligence artificielle tient dans sa capacité à exécuter des tâches, efficacement et rapidement, sans repos. Pour ce faire, les IA doivent pouvoir agir de manière autonome. Plus elles peuvent agir dans le monde, plus elles sont utiles. Plus elles sont intelligentes, créatives et autonomes, plus elles peuvent agir dans le monde.
</ReportSection>

<ReportSection id="bibliographie" navTitle="Bibliographie">
[1]	[K. Bryant, “How AI Is Impacting Society And Shaping The Future,” Forbes Magazine, Dec. 13, 2023. Available: https://www.forbes.com/sites/kalinabryant/2023/12/13/how-ai-is-impacting-society-and-shaping-the-future/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/PKULH)


    [2]	[A. Jindal and A. Sharma, “THE IMPACT OF ARTIFICIAL INTELLIGENCE ON SOCIETY,” Mar. 18, 2024. doi: 10.13140/RG.2.2.18522.96968. Available: https://www.researchgate.net/publication/379038753_THE_IMPACT_OF_ARTIFICIAL_INTELLIGENCE_ON_SOCIETY. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/Ad0eu)


    [3]	[P. Aghion and A. Bouverot, IA : notre ambition pour la France. Odile Jacob, 2024.](http://paperpile.com/b/V0trIA/3d3fm)


    [4]	[C. Blanchot, “« Il est inconcevable que le déploiement de l’intelligence artificielle se fasse sans débat public et sans évaluation de son impact sur notre travail »,” Le Monde, Le Monde, May 16, 2024. Available: https://www.lemonde.fr/idees/article/2024/05/16/il-est-inconcevable-que-le-deploiement-de-l-intelligence-artificielle-se-fasse-sans-debat-public-et-sans-evaluation-de-son-impact-sur-notre-travail_6233566_3232.html. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/OxGj7)


    [5]	[“Pour une IA française tournée vers l’avenir.” Available: https://www.securite-ia.fr/post/pour-une-ia-francaise-tournee-vers-lavenir. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/09bm)


    [6]	[Wikipedia contributors, “AI safety,” Wikipedia, The Free Encyclopedia, Aug. 11, 2024. Available: https://en.wikipedia.org/w/index.php?title=AI_safety&oldid=1239791378](http://paperpile.com/b/V0trIA/Xy4dz)


    [7]	[S. Russell, Human Compatible: AI and the Problem of Control. Penguin UK, 2019.](http://paperpile.com/b/V0trIA/zp3pc)


    [8]	[J. Mecklin, “‘AI Godfather’ Yoshua Bengio: We need a humanity defense organization,” Bulletin of the Atomic Scientists, Oct. 17, 2023. Available: https://thebulletin.org/2023/10/ai-godfather-yoshua-bengio-we-need-a-humanity-defense-organization/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/hnEIi)


    [9]	[“Reasoning through arguments against taking AI safety seriously,” Yoshua Bengio, Jul. 09, 2024. Available: https://yoshuabengio.org/2024/07/09/reasoning-through-arguments-against-taking-ai-safety-seriously/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/JT1YF)


    [10]	[“Video: Geoffrey Hinton talks about the ‘existential threat’ of AI,” MIT Technology Review, May 03, 2023. Available: https://www.technologyreview.com/2023/05/03/1072589/video-geoffrey-hinton-google-ai-risk-ethics/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/c8QZ8)


    [11]	[S. Cave, “Risks from Artificial Intelligence.” Available: https://www.cser.ac.uk/research/risks-from-artificial-intelligence. [Accessed: Aug. 19, 2024]](http://paperpile.com/b/V0trIA/BtfX)


    [12]	[“Center for AI safety (CAIS).” Available: https://www.safe.ai/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/qdJY)


    [13]	[“Center for Human-Compatible Artificial Intelligence – Center for Human-Compatible AI is building exceptional AI for humanity.” Available: https://humancompatible.ai/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/mcNh)


    [14]	[F. Gustafsson, “Automotive safety systems,” IEEE Signal Process. Mag., vol. 26, no. 4, pp. 32–47, Jul. 2009.](http://paperpile.com/b/V0trIA/oaxP)


    [15]	[“Introducing Superalignment.” Available: https://openai.com/index/introducing-superalignment/. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/ouOj)


    [16]	[K. Wiggers, “Google DeepMind forms a new org focused on AI safety,” TechCrunch, Feb. 21, 2024. Available: https://techcrunch.com/2024/02/21/google-deepmind-forms-a-new-org-focused-on-ai-safety/. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/oKud)


    [17]	[“Anca Dragan named Head of AI Safety and Alignment at Google DeepMind,” EECS at Berkeley, Mar. 28, 2024. Available: https://eecs.berkeley.edu/news/anca-dragan-named-head-of-ai-safety-and-alignment-at-google-deepmind. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/n7qT)


    [18]	[“Responsibility & safety,” Google DeepMind. Available: https://deepmind.google/about/responsibility-safety/. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/oFEj)


    [19]	[K. Wiggers, “Anthropic hires former OpenAI safety lead to head up new team,” TechCrunch, May 28, 2024. Available: https://techcrunch.com/2024/05/28/anthropic-hires-former-openai-safety-lead-to-head-up-new-team/. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/42xA)


    [20]	[“Research.” Available: https://www.anthropic.com/research. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/YXCg)


    [21]	[“Stanford AI safety.” Available: https://aisafety.stanford.edu/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/nGAW)


    [22]	[“MIT AI alignment,” MIT AI Alignment. Available: https://www.mitalignment.org/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/Ld7M)


    [23]	[“SafeAI: Safe Artificial Intelligence.” Available: https://safeai.ethz.ch/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/7Otq)


    [24]	[“Turing seminar – an introduction to AGI safety – master MVA.” Available: https://www.master-mva.com/cours/seminaire-turing/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/IpSsj)


    [25]	[“Inauguration de la Chaire « Intelligence artificielle de confiance et responsable »,” École polytechnique, école d’ingénieur. Available: https://www.polytechnique.edu/actualites/inauguration-de-la-chaire-intelligence-artificielle-de-confiance-et-responsable. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/lLTSI)


    [26]	[Wikipedia contributors, “Reinforcement learning from human feedback,” Wikipedia, The Free Encyclopedia, Jul. 13, 2024. Available: https://en.wikipedia.org/w/index.php?title=Reinforcement_learning_from_human_feedback&oldid=1234302797](http://paperpile.com/b/V0trIA/pvRc)


    [27]	[P. Christiano, J. Leike, T. B. Brown, M. Martic, S. Legg, and D. Amodei, “Deep reinforcement learning from human preferences,” arXiv [stat.ML], Jun. 12, 2017. Available: http://arxiv.org/abs/1706.03741](http://paperpile.com/b/V0trIA/nZwI)


    [28]	[“Specification gaming: the flip side of AI ingenuity,” Google DeepMind. Available: https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/Pv5mO)


    [29]	[N. Nanda, “A comprehensive mechanistic interpretability explainer & glossary,” Neel Nanda, Dec. 21, 2022. Available: https://www.neelnanda.io/mechanistic-interpretability/glossary. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/b0Bl)


    [30]	[A. Templeton, Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Anthropic, 2024.](http://paperpile.com/b/V0trIA/URfna)


    [31]	[D. Hendrycks, M. Mazeika, and T. Woodside, “An Overview of Catastrophic AI Risks,” arXiv [cs.CY], Jun. 21, 2023. Available: http://arxiv.org/abs/2306.12001](http://paperpile.com/b/V0trIA/Auk9N)


    [32]	[L. Weidinger et al., “Ethical and social risks of harm from Language Models,” arXiv [cs.CL], Dec. 08, 2021. Available: http://arxiv.org/abs/2112.04359](http://paperpile.com/b/V0trIA/Kp1pE)


    [33]	[“Panorama,” Kardiologe, vol. 3, no. 3, pp. 192–193, Jun. 2009.](http://paperpile.com/b/V0trIA/6LW2Z)


    [34]	[“2022 expert survey on progress in AI,” AI Impacts, Aug. 04, 2022. Available: https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/ghKix)


    [35]	[Center for AI Safety, “Statement on AI risk,” CAIS | Center for AI Safety. Available: https://www.safe.ai/work/statement-on-ai-risk. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/eRWRE)


    [36]	[“Yoshua Bengio.” Available: https://amturing.acm.org/award_winners/bengio_3406375.cfm. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/npzJ)


    [37]	[“Geoffrey E. Hinton,” in Talking Nets, The MIT Press, 2000.](http://paperpile.com/b/V0trIA/1Rxe)


    [38]	[“Pause giant AI experiments: An open letter,” Future of Life Institute, Mar. 22, 2023. Available: https://futureoflife.org/open-letter/pause-giant-ai-experiments/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/WW03B)


    [39]	[“4 Charts That Show Why AI Progress Is Unlikely to Slow Down,” Time. Available: https://time.com/6300942/ai-progress-charts/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/zJYT7)


    [40]	[“AI index report 2024 – artificial intelligence index.” Available: https://aiindex.stanford.edu/report/. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/oULXh)


    [41]	[“International scientific report on the safety of advanced AI,” GOV.UK, May 17, 2024. Available: https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/2ept8)


    [42]	[M. K. Cohen, N. Kolt, Y. Bengio, G. K. Hadfield, and S. Russell, “Regulating advanced artificial agents,” Science, vol. 384, no. 6691, pp. 36–38, Apr. 2024.](http://paperpile.com/b/V0trIA/7UVQb)


    [43]	_[AutoGPT: AutoGPT is the vision of accessible AI for everyone, to use and to build on. Our mission is to provide the tools, so that you can focus on what matters. Github. Available: https://github.com/Significant-Gravitas/AutoGPT. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/ccDcs)_


    [44]	[“Cognition.” Available: https://www.cognition.ai/blog/introducing-devin. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/QqYJn)


    [45]	[“Genie: SOTA Software engineering model.” Available: https://cosine.sh/genie. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/xkIrC)


    [46]	[C. Lu, C. Lu, R. T. Lange, J. Foerster, J. Clune, and D. Ha, “The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery,” arXiv [cs.AI], Aug. 12, 2024. Available: http://arxiv.org/abs/2408.06292](http://paperpile.com/b/V0trIA/ybkz)


    [47]	[“Capital industriel moderne, demande de travail et dynamique des marchés de produits : le cas de la France.” Available: https://www.insee.fr/fr/statistiques/7613948. [Accessed: Aug. 11, 2024]](http://paperpile.com/b/V0trIA/lK3GH)


    [48]	[Wikipedia contributors, “Intelligence artificielle,” Wikipedia, The Free Encyclopedia. Available: https://fr.wikipedia.org/w/index.php?title=Intelligence_artificielle&oldid=217518957](http://paperpile.com/b/V0trIA/58rlx)


    [49]	[Wikipedia contributors, “Deep learning,” Wikipedia, The Free Encyclopedia, Aug. 12, 2024. Available: https://en.wikipedia.org/w/index.php?title=Deep_learning&oldid=1239902356](http://paperpile.com/b/V0trIA/VThFP)


    [50]	[Wikipedia contributors, “Large language model,” Wikipedia, The Free Encyclopedia, Aug. 12, 2024. Available: https://en.wikipedia.org/w/index.php?title=Large_language_model&oldid=1239909148](http://paperpile.com/b/V0trIA/kqmAC)


    [51]	[J. Wei et al., “Emergent Abilities of Large Language Models,” arXiv [cs.CL], Jun. 15, 2022. Available: http://arxiv.org/abs/2206.07682](http://paperpile.com/b/V0trIA/Mm57s)


    [52]	[R. Ngo, “Visualizing the deep learning revolution - Richard Ngo,” Medium, Jan. 05, 2023. Available: https://medium.com/@richardcngo/visualizing-the-deep-learning-revolution-722098eb9c5. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/PG4SP)


    [53]	[J. E. Dobson, “On reading and interpreting black box deep neural networks,” International Journal of Digital Humanities, vol. 5, no. 2, pp. 431–449, Nov. 2023.](http://paperpile.com/b/V0trIA/d33wU)


    [54]	[L. Bereska and E. Gavves, “Mechanistic Interpretability for AI Safety -- A Review,” arXiv [cs.AI], Apr. 22, 2024. Available: http://arxiv.org/abs/2404.14082](http://paperpile.com/b/V0trIA/nfPl)


    [55]	[T. Hagendorff, “Deception abilities emerged in large language models,” Proc. Natl. Acad. Sci. U. S. A., vol. 121, no. 24, p. e2317967121, Jun. 2024.](http://paperpile.com/b/V0trIA/6mKdU)


    [56]	[J. Vincent, “Microsoft’s Bing is an emotionally manipulative liar, and people love it,” The Verge, Feb. 15, 2023. Available: https://www.theverge.com/2023/2/15/23599072/microsoft-ai-bing-personality-conversations-spy-employees-webcams. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/sRfym)


    [57]	[R. Fang, R. Bindu, A. Gupta, Q. Zhan, and D. Kang, “LLM Agents can Autonomously Hack Websites,” arXiv [cs.CR], Feb. 06, 2024. Available: http://arxiv.org/abs/2402.06664](http://paperpile.com/b/V0trIA/WhQyF)


    [58]	[R. Fang, R. Bindu, A. Gupta, Q. Zhan, and D. Kang, “Teams of LLM Agents can Exploit Zero-Day Vulnerabilities,” arXiv [cs.MA], Jun. 02, 2024. Available: http://arxiv.org/abs/2406.01637](http://paperpile.com/b/V0trIA/g9wcP)


    [59]	[R. Fang, R. Bindu, A. Gupta, and D. Kang, “LLM Agents can Autonomously Exploit One-day Vulnerabilities,” arXiv [cs.CR], Apr. 11, 2024. Available: http://arxiv.org/abs/2404.08144](http://paperpile.com/b/V0trIA/L5AsK)


    [60]	[M. Shao et al., “An Empirical Evaluation of LLMs for Solving Offensive Security Challenges,” arXiv [cs.CR], Feb. 19, 2024. Available: http://arxiv.org/abs/2402.11814](http://paperpile.com/b/V0trIA/AgWnR)


    [61]	[D. Kiela, “Plotting progress in AI,” Contextual AI, Jul. 31, 2023. Available: https://contextual.ai/news/plotting-progress-in-ai/. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/FZgwJ)


    [62]	[J. Kaplan et al., “Scaling Laws for Neural Language Models,” arXiv [cs.LG], Jan. 23, 2020. Available: http://arxiv.org/abs/2001.08361](http://paperpile.com/b/V0trIA/3HmY)


    [63]	[“Large-scale AI models,” Epoch AI, Jun. 19, 2024. Available: https://epochai.org/data/large-scale-ai-models. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/fObp)


    [64]	[“OpenAI Charter.” Available: https://openai.com/charter/. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/YieF9)


    [65]	[“SITUATIONAL AWARENESS: The decade ahead,” SITUATIONAL AWARENESS - The Decade Ahead, May 29, 2024. Available: https://situational-awareness.ai/. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/ASqR)


    [66]	[“When Might AI Outsmart Us? It Depends Who You Ask,” Time. Available: https://time.com/6556168/when-ai-outsmart-humans/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/rC9t)


    [67]	[P. Gmyrek, J. Berg, and D. Bescond, “Generative AI and jobs: A global analysis of potential effects on job quantity and quality,” SSRN Electron. J., Aug. 2023, doi: 10.2139/ssrn.4584219. Available: https://www.ilo.org/publications/generative-ai-and-jobs-global-analysis-potential-effects-job-quantity-and. [Accessed: Aug. 13, 2024]](http://paperpile.com/b/V0trIA/pPX4x)


    [68]	[C. Pizzinelli, A. J. Panton, M. M. M. Tavares, M. Cazzaniga, and L. Li, Labor Market Exposure to AI: Cross-country Differences and Distributional Implications. International Monetary Fund, 2023.](http://paperpile.com/b/V0trIA/B5SYZ)


    [69]	[“The state of phishing 2024,” SlashNext | Complete Generative AI Security for Email, Mobile, and Browser, May 17, 2024. Available: https://slashnext.com/the-state-of-phishing-2024/. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/Q76Dv)


    [70]	[J. Hazell, “Spear Phishing With Large Language Models,” arXiv [cs.CY], May 11, 2023. Available: http://arxiv.org/abs/2305.06972](http://paperpile.com/b/V0trIA/P5tbl)


    [71]	[D. Milmo, “Company worker in Hong Kong pays out £20m in deepfake video call scam,” The Guardian, The Guardian, Feb. 05, 2024. Available: https://www.theguardian.com/world/2024/feb/05/hong-kong-company-deepfake-video-conference-call-scam. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/Fss8R)


    [72]	[G. Boesch, “What Is Adversarial Machine Learning? Attack Methods in 2024,” viso.ai, Dec. 01, 2023. Available: https://viso.ai/deep-learning/adversarial-machine-learning/. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/aMgnc)


    [73]	[“Training data extraction attacks.” Available: https://www.nightfall.ai/ai-security-101/training-data-extraction-attacks. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/U4I99)


    [74]	[“The near-term impact of AI on the cyber threat.” Available: https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/2S4qr)


    [75]	[“World Economic Forum,” World Economic Forum, Jan. 18, 2023. Available: https://www.weforum.org/publications/global-cybersecurity-outlook-2023/. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/LpQ77)


    [76]	[“Voice of SecOps reports,” Deep Instinct, May 12, 2022. Available: https://www.deepinstinct.com/voice-of-secops-reports. [Accessed: Aug. 12, 2024]](http://paperpile.com/b/V0trIA/4H8x2)


    [77]	[“Open-sourcing highly capable foundation models.” Available: https://www.governance.ai/research-paper/open-sourcing-highly-capable-foundation-models. [Accessed: Aug. 13, 2024]](http://paperpile.com/b/V0trIA/z6cbi)


    [78]	[R. Slayton, “Conceptions, Causes, and Assessment,” Int. Secur., vol. 41, no. 3, pp. 72–109, 2016.](http://paperpile.com/b/V0trIA/pckBE)


    [79]	_[Defending a New Domain: The Pentagon’s Cyberstrategy. Defense Technical Information Center, 2010.](http://paperpile.com/b/V0trIA/nABiG)_


    [80]	[P. Venables and C. Snyder, “Cloud CISO Perspectives: Building better cyber defenses with AI,” Google Cloud Blog, Feb. 29, 2024. Available: https://cloud.google.com/blog/products/identity-security/cloud-ciso-perspectives-building-better-cyber-defenses-with-ai/. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/Zgy6)


    [81]	[P. Gade, S. Lermen, C. Rogers-Smith, and J. Ladish, “BadLlama: cheaply removing safety fine-tuning from Llama 2-Chat 13B,” arXiv [cs.CL], Oct. 31, 2023. Available: http://arxiv.org/abs/2311.00117](http://paperpile.com/b/V0trIA/zYwzv)


    [82]	[“Disrupting deceptive uses of AI by covert influence operations.” Available: https://openai.com/index/disrupting-deceptive-uses-of-AI-by-covert-influence-operations/. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/zUYC)


    [83]	[C. Mouton, C. Lucas, and E. Guest, The Operational Risks of AI in Large-Scale Biological Attacks. RAND Corporation, 2024.](http://paperpile.com/b/V0trIA/e8nXr)


    [84]	[“Reddit - Dive into anything.” Available: https://www.reddit.com/r/ArtistHate/. [Accessed: Aug. 13, 2024]](http://paperpile.com/b/V0trIA/pv0Kb)


    [85]	[J. Wolters, “The animation guild: Future unscripted: The Impact of Generative AI on entertainment industry jobs,” INDAC, Feb. 01, 2024. Available: https://indac.org/blog/the-animation-guild-future-unscripted-the-impact-of-generative-ai-on-entertainment-industry-jobs/. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/sLQT5)


    [86]	[“The potentially large effects of artificial intelligence on economic growth (Briggs/kodnani).” Available: https://www.gspublishing.com/content/research/en/reports/2023/03/27/d64e052b-0f6e-45d7-967b-d7be35fabd16.html. [Accessed: Aug. 13, 2024]](http://paperpile.com/b/V0trIA/ECzq5)


    [87]	[“Sondage Ifop - Talan : les Français et les IA génératives,” May 11, 2023. Available: https://www.talan.com/actualites/detail-actualites/news/sondage-ifop-talan-les-francais-et-les-ia-generatives/. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/57LHE)


    [88]	[“OpinionWay pour Universcience - Barometre de l’esprit critique 2024 - Mars 2024.” Available: https://www.opinion-way.com/fr/sondage-d-opinion/sondages-publies/opinion-societe/opinionway-pour-universcience-barometre-de-l-esprit-critique-2024-mars-2024.html. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/wb9p2)


    [89]	[“Intelligence artificielle : un Français sur deux inquiet,” BCG Global. Available: https://www.bcg.com/press/26april2024-intelligence-artificielle-un-francais-sur-deux-inquiet. [Accessed: Aug. 14, 2024]](http://paperpile.com/b/V0trIA/KfPuW)


    [90]	[Y. Yao, J. Duan, K. Xu, Y. Cai, Z. Sun, and Y. Zhang, “A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly,” arXiv [cs.CR], Dec. 04, 2023. Available: http://arxiv.org/abs/2312.02003](http://paperpile.com/b/V0trIA/mtD8)


    [91]	[“AI Safety Summit,” AISS 2023, Sep. 27, 2023. Available: https://www.aisafetysummit.gov.uk/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/fsuS)


    [92]	[“The bletchley declaration by countries attending the AI safety summit, 1-2 November 2023,” GOV.UK, Nov. 01, 2023. Available: https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/fq98)


    [93]	[“Action Plan to increase the safety and security of advanced AI.” Available: https://www.gladstone.ai/action-plan. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/i9pT)


    [94]	[R. J. Neuwirth, The EU artificial intelligence act: Regulating subliminal AI systems. in Routledge Research in the Law of Emerging Technologies. London, England: Routledge, 2022. Available: https://artificialintelligenceact.eu/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/qEt2)


    [95]	[The White House, “Executive order on the safe, secure, and trustworthy development and use of artificial intelligence,” The White House, Oct. 30, 2023. Available: https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/wgii)


    [96]	[“International scientific report on the safety of advanced AI,” GOV.UK, May 17, 2024. Available: https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/C2ny)


    [97]	[T. Tech, “Anne Bouverot : La France, capitale européenne de l’iA ?,” Jul. 10, 2024. Available: https://www.youtube.com/watch?v=oQJhqxnDsjo. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/LiGM)


    [98]	[F. Brewster, “Big Tech is lobbying hard to keep copyright law favorable to AI.” Available: https://jacobin.com/2023/11/artificial-intelligence-big-tech-lobbying-copyright-infringement-regulation/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/m5C2C)


    [99]	[“TTP - funding the fight against antitrust: How Facebook’s antiregulatory attack dog spends its millions.” Available: https://www.techtransparencyproject.org/articles/funding-fight-against-antitrust-how-facebooks-antiregulatory-attack-dog-spends-its-millions. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/TyPM)


    [100]	[S. Hashim, “Meta-funded group floods Facebook with anti-AI regulation ads,” Transformer, Jul. 15, 2024. Available: https://www.transformernews.ai/p/american-edge-meta-ai-regulation-lobbying. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/CW0Vf)


    [101]	[S. Hashim, “Tech companies are trying to kill California’s AI regulation bill,” Transformer, Jul. 10, 2024. Available: https://www.transformernews.ai/p/a16z-y-combinator-big-tech-sb1047-lobbying. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/HAX6j)


    [102]	[“Trojan horses: how European startups teamed up with Big Tech to gut the AI Act.” Available: https://corporateeurope.org/en/2024/03/trojan-horses-how-european-startups-teamed-big-tech-gut-ai-act. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/SJguQ)


    [103]	[S. User, “Yann Le Cun.” Available: https://www.academie-sciences.fr/fr/Liste-des-membres-de-l-Academie-des-sciences-/-L/yann-le-cun.html. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/hLcuz)


    [104]	[O. LinkedIn, “About linkedin,” LinkedIn Corporation, 2022, Available: https://www.linkedin.com/posts/yann-lecun_today-i-was-made-a-chevalier-de-la-l%C3%A9gion-activity-7138326352776056832-GQFp/](http://paperpile.com/b/V0trIA/HeJKh)


    [105]	[“[No title],” X (formerly Twitter). Available: https://x.com/ylecun. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/KG7rp)


    [106]	[“LinkedIn.” Available: https://www.linkedin.com/in/yann-lecun/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/PzRyk)


    [107]	[A. R. Chow, “Yann LeCun,” Time, Sep. 07, 2023. Available: https://time.com/collection/time100-ai/6309052/yann-lecun/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/4WMYq)


    [108]	[“LinkedIn.” Available: https://www.linkedin.com/posts/yann-lecun_current-and-former-colleagues-of-the-french-activity-7200524405267988480-NyaA/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/8FO1b)


    [109]	[“[No title],” X (formerly Twitter). Available: https://x.com/ylecun/status/1747714991292399763. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/xA2H1)


    [110]	[“Yann LeCun - Our CTO, Mike Schroepfer, gives the details of the.” Available: https://www.facebook.com/yann.lecun/posts/10155043225132143. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/eP6AK)


    [111]	[“[No title],” X (formerly Twitter). Available: https://x.com/ylecun/status/1688140941079498752. [Accessed: Aug. 18, 2024]](http://paperpile.com/b/V0trIA/HB33)


    [112]	[“Myanmar: Time for Meta to pay reparations to Rohingya for role in ethnic cleansing,” Amnesty International, Aug. 25, 2023. Available: https://www.amnesty.org/en/latest/news/2023/08/myanmar-time-for-meta-to-pay-reparations-to-rohingya-for-role-in-ethnic-cleansing/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/xkf8)


    [113]	[“Ethiopia: Meta’s failures contributed to abuses against Tigrayan community during conflict in northern Ethiopia,” Amnesty International, Oct. 31, 2023. Available: https://www.amnesty.org/en/latest/news/2023/10/meta-failure-contributed-to-abuses-against-tigray-ethiopia/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/Xvvf)


    [114]	[D. Lauer, “Facebook’s ethical failures are not accidental; they are part of the business model,” AI Ethics, vol. 1, no. 4, pp. 395–403, Jun. 2021.](http://paperpile.com/b/V0trIA/39kA)


    [115]	[I. A. Hamilton, “‘You just have the wrong idea about Facebook’: Facebook’s chief AI scientist defends the company over Black Lives Matter uproar,” Business Insider, Jun. 09, 2020. Available: https://www.businessinsider.com/facebook-chief-ai-scientist-yann-lecun-blm-defence-2020-6. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/0tav)


    [116]	[T. Greene, “Facebook AI boss Yann LeCun goes off in Twitter rant, blames talk radio for hate content,” The Next Web, Mar. 12, 2021. Available: https://thenextweb.com/news/facebook-ai-boss-yann-lecun-goes-off-in-twitter-rant-blames-talk-radio-for-hate-content. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/M0XB)


    [117]	[A. I. Mistral, “Mistral AI.” Available: https://mistral.ai/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/BMoE)


    [118]	[K.D, “IA: la start-up française Mistral AI valorisée 6 milliards d’euros après une nouvelle levée de fonds,” Tech&Co, Jun. 11, 2024. Available: https://www.bfmtv.com/tech/intelligence-artificielle/ia-la-startup-francaise-mistral-ai-valorisee-6-milliards-d-euros-apres-une-nouvelle-levee-de-fonds_AD-202406110709.html. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/6L6k8)


    [119]	[S. Pommier, “L’ex-ministre Cédric 0 pourrait empocher 23 millions d'euros après avoir investi 176 euros dans Mistral AI,” Capital.fr, Dec. 12, 2023. Available: https://www.capital.fr/economie-politique/mistral-ai-la-bonne-affaire-du-conseiller-fondateur-cedric-o-1488506. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/7QeEk)


    [120]	[Z. Wanat, “‘EU’s AI act could kill our company,’ says Mistral’s Cédric O,” Sifted, Aug. 06, 2024. Available: https://sifted.eu/articles/eu-ai-act-kill-mistral-cedric-o. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/ghIWL)


    [121]	[C. Axiotes, “Lobbying for loopholes: The battle over foundation models in the EU AI act,” EURACTIV, Nov. 24, 2023. Available: https://www.euractiv.com/section/digital/opinion/lobbying-for-loopholes-the-battle-over-foundation-models-in-the-eu-ai-act/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/fzErG)


    [122]	[“[No title],” X (formerly Twitter). Available: https://x.com/tegmark/status/1728851164291059948. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/62cK6)


    [123]	[I. Ramdani, “Intelligence artificielle : comment un ministre devenu lobbyiste a retourné le gouvernement,” Mediapart, Dec. 01, 2023. Available: https://www.mediapart.fr/journal/france/011223/intelligence-artificielle-comment-un-ministre-devenu-lobbyiste-retourne-le-gouvernement. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/JV9a8)


    [124]	[S. Pommier and S. Barge, “Régulation de l’IA : l'ex-secrétaire d'Etat Cédric O jongle entre ses intérêts et ceux de la France,” Capital.fr, Feb. 07, 2024. Available: https://www.capital.fr/economie-politique/regulation-de-lia-lex-secretaire-detat-cedric-o-jongle-entre-ses-interets-et-ceux-de-la-france-1491749. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/C8c3P)


    [125]	[T. Hartmann, “Les coulisses de l’opposition de la France à la réglementation des modèles d’IA,” EURACTIV, Nov. 29, 2023. Available: https://www.euractiv.fr/section/intelligence-artificielle/news/les-coulisses-de-lopposition-de-la-fance-a-la-reglementation-des-modeles-dia/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/G086D)


    [126]	[T. Hartmann, “AI Act : le gouvernement accusé d’avoir été influencé par un lobbyiste en situation de conflit d’intérêts,” EURACTIV, Dec. 21, 2023. Available: https://www.euractiv.fr/section/intelligence-artificielle/news/ai-act-le-gouvernement-accuse-davoir-ete-influence-par-un-lobbyiste-en-situation-de-conflit-dinterets/. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/GKV6c)


    [127]	[“Open letter EU AI act and signatories.Pdf,” Google Docs. Available: https://drive.google.com/file/d/1wrtxfvcD9FwfNfWGDL37Q6Nd8wBKXCkn/view. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/MEVyu)


    [128]	[L. Bertuzzi, “AI Act : négociations bloquées à cause de divergences sur les modèles de fondation,” EURACTIV, Nov. 13, 2023. Available: https://www.euractiv.fr/section/intelligence-artificielle/news/ai-act-negociations-bloquees-a-cause-de-divergences-sur-les-modeles-de-fondation/?_ga=2.50197092.31701998.1701074244-1669050714.1683052757. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/5Hkbm)


    [129]	[Propos recueillis par Philippe Mabille et Sylvain Rolland au Forum AIM Marseille, “Intelligence artificielle : « Les Gafam et la startup Mistral ne défendent pas l’intérêt général » (Thierry Breton),” La Tribune, Nov. 24, 2023. Available: https://www.latribune.fr/technos-medias/informatique/intelligence-artificielle-les-gafam-et-la-startup-mistral-ne-defendent-pas-l-interet-general-thierry-breton-984046.html. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/lKYza)


    [130]	[O. Cédric, “This is worth some clarification. Before I do, Mr @tegmark, I shall say how pathetic I find your original tweet. You can obviously deeply disagree with my point of view, but the way you phrased it on a personal attack on my probity is to me epitomizes the decay of public debate.…,” Twitter, Nov. 26, 2023. Available: https://twitter.com/cedric_o/status/1728724005459235052. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/vBuDf)


    [131]	[“[No title],” X (formerly Twitter). Available: https://x.com/a16z/status/1720524920596128012. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/WP89S)


    [132]	[V. Hassija et al., “Interpreting Black-Box Models: A Review on Explainable Artificial Intelligence,” Cognit. Comput., vol. 16, no. 1, pp. 45–74, Jan. 2024.](http://paperpile.com/b/V0trIA/aOeZD)


    [133]	[“[No title],” X (formerly Twitter). Available: https://x.com/edardaman/status/1744453999695176046. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/YHZpB)


    [134]	[“[No title],” X (formerly Twitter). Available: https://x.com/NeelNanda5/status/1799203292066558403. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/s0mh7)


    [135]	[“[No title],” X (formerly Twitter). Available: https://x.com/m_bourgon/status/1798863250173657312. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/rRolZ)


    [136]	[“[No title],” X (formerly Twitter). Available: https://x.com/ID_AA_Carmack/status/1799147185793348006. [Accessed: Aug. 15, 2024]](http://paperpile.com/b/V0trIA/2Nmax)


    [137]	[Sénat, “Intelligence artificielle : la France en pointe ?,” May 22, 2024. Available: https://www.youtube.com/watch?v=tWXuGuOHMhk. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/bJmQ)


    [138]	[T. Petersen, “AI’s new power of persuasion: it can change your mind,” Apr. 2024, Available: https://actu.epfl.ch/news/ai-s-new-power-of-persuasion-it-can-change-your-mi/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/ZlKN)


    [139]	[F. Xu, H. Uszkoreit, Y. Du, W. Fan, D. Zhao, and J. Zhu, “Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges,” in Natural Language Processing and Chinese Computing, Springer International Publishing, 2019, pp. 563–574.](http://paperpile.com/b/V0trIA/pd9L)


    [140]	[“Guardrailing.” Available: https://docs.mistral.ai/capabilities/guardrailing/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/hibG)


    [141]	[Y. Wolf, N. Wies, O. Avnery, Y. Levine, and A. Shashua, “Fundamental Limitations of Alignment in Large Language Models,” arXiv [cs.CL], Apr. 19, 2023. Available: http://arxiv.org/abs/2304.11082](http://paperpile.com/b/V0trIA/NsqZ)


    [142]	[“Aligning language models to follow instructions.” Available: https://openai.com/index/instruction-following/. [Accessed: Aug. 16, 2024]](http://paperpile.com/b/V0trIA/8PKu)


    [143]	[J. Ji et al., “Language Models Resist Alignment,” arXiv [cs.CL], Jun. 10, 2024. Available: http://arxiv.org/abs/2406.06144](http://paperpile.com/b/V0trIA/z6cl)


    [144]	[Z. Kenton, T. Everitt, L. Weidinger, I. Gabriel, V. Mikulik, and G. Irving, “Alignment of Language Agents,” arXiv [cs.AI], Mar. 26, 2021. Available: http://arxiv.org/abs/2103.14659](http://paperpile.com/b/V0trIA/K6s1)
</ReportSection>

